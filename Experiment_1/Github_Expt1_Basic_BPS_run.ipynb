{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Github_Expt1_Basic_BPS_run.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP6G1Uruj2itTGZ0K4wRJ07"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8eB1MxByioS6","colab_type":"text"},"source":["# Initialise dim-50 standard Gaussian dist"]},{"cell_type":"code","metadata":{"id":"MzAbxZ8KIZtJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Qm6yMQCiwTH","colab_type":"code","colab":{}},"source":["dim = 50\n","\n","Stand_Gaus_50 = Gaussian(np.zeros(dim), np.identity(dim))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYv41GYZje4C","colab_type":"text"},"source":["# Initialise starting points"]},{"cell_type":"markdown","metadata":{"id":"1KJbkky4mMAW","colab_type":"text"},"source":["Here is how I initialise starting positions:"]},{"cell_type":"code","metadata":{"id":"36qrP7yMjg2U","colab_type":"code","colab":{}},"source":["partial = np.identity(dim)\n","starting_pts = np.concatenate((np.array([[0]*dim]), 1*partial, 2*partial, 3*partial, \n","                               (-1)*partial, (-2)*partial, (-3)*partial), axis=0)\n","\n","another_half = np.concatenate((partial[1:], np.array([[1] + [0]*(dim-1)])), axis=0) + partial\n","\n","starting_pts = np.concatenate((starting_pts, another_half, 2*another_half, 3*another_half, \n","                               (-1)*another_half, (-2)*another_half, (-3)*another_half), axis=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-17bqgGkxwJ","colab_type":"text"},"source":["# Run BPS algorithm on Gaussian. Save the data of the trajectory:\n","\n","1. Turning points\n","2. Velocities there\n","3. The Markov chain's time there\n","4. CPU time spent\n","5. pdf evaluations done"]},{"cell_type":"code","metadata":{"id":"wQfgXsUtk7Ys","colab_type":"code","colab":{}},"source":["n_batch = 0\n","time_trajectory = 100000\n","lambda_ref = 1\n","prob_dist = Stand_Gaus_50\n","\n","\n","for i in range(20):\n","  start_x = starting_pts[100*n_batch + i]\n","  if np.linalg.norm(start_x) == 0:\n","    start_v = np.array([1] + [0]*(dim-1))\n","  else:\n","    start_v = start_x\n","  \n","  turn_pts, list_of_velo, striding_times, total_evals_list, computational_times\\\n","  = BPS_basic(x0 = start_x, v0 = start_v, Time = time_trajectory, lambda_ref = lambda_ref, \n","              prob_dist = prob_dist)\n","\n","\n","  turning_points = np.array(turn_pts)\n","  v_list = np.array(list_of_velo)\n","  stride_times = np.array(striding_times)\n","  evaluations_list = np.array(total_evals_list)\n","  CPUtime_list = np.array(computational_times)\n","\n","\n","\n","  path_to_save_chain = 'BPS_data/BPS_chain_' + str(100*n_batch + i) + '_'\n","  np.savez(path_to_save_chain + 'turning_points', turning_points = turning_points)\n","  np.savez(path_to_save_chain + 'v_list', v_list = v_list)\n","  np.savez(path_to_save_chain + 'stride_times', stride_times = stride_times)\n","  np.savez(path_to_save_chain + 'evaluations_list', evaluations_list = evaluations_list)\n","  np.savez(path_to_save_chain + 'CPUtime_list', CPUtime_list = CPUtime_list)\n","\n","  \n","  print('finishes the {}th run of BPS chain    cpu_time: {} sec'.format(100*n_batch + i, \n","                                                                        computational_times[-1]))\n","\n","\n","print('finishes executing a whole batch of BPS chains!')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wijHRBa4OLKb","colab_type":"text"},"source":["# Compute the positions and velocities at time = 0, 100, ..., 28000 (or further) of the trajectory.\n","\n","# Save such data."]},{"cell_type":"code","metadata":{"id":"LXRi9bbF_vla","colab_type":"code","colab":{}},"source":["# plot_step_size means that I would like to extract the velocity and position \n","# at time=100 of the trajectory.\n","\n","plot_step_size = 100\n","n_batch = 0\n","num_files = 20\n","starting_file = 0\n","all_chains = []\n","\n","start_time = perf_counter()\n","\n","for i in range(num_files):\n","  \n","  path_to_load_chain = 'BPS_data/BPS_chain_' + str(100*n_batch + starting_file + i) + '_'\n","  x_load = np.load(path_to_load_chain + 'turning_points.npz')\n","  v_load = np.load(path_to_load_chain + 'v_list.npz')\n","  t_load = np.load(path_to_load_chain + 'stride_times.npz')\n","\n","  x = x_load['turning_points']\n","  v = v_load['v_list']\n","  t = t_load['stride_times']\n","\n","  interm_times = np.arange(0,100000.1,plot_step_size)\n","\n","  x_100s, v_100s, t_100s = x_v_t_arbitrary_times(turn_pts=x, list_of_velo=v, striding_times=t, intermediate_times=interm_times)\n","\n","\n","  path_to_save_chain = 'processed_BPS_data/processed_BPS_chain_' + str(100*n_batch + starting_file + i) + '_'\n","  np.savez(path_to_save_chain + 'x_array', x_array = np.array(x_100s))\n","  np.savez(path_to_save_chain + 'v_array', v_array = np.array(v_100s))\n","\n","  print('finishes_processing {}th chain    cpu_time: {} sec'.format(100*n_batch + starting_file + i, perf_counter() - start_time))\n","\n","\n","print('finishes processing all!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-6__gePO22_","colab_type":"text"},"source":["# Computation of KL divergences of the 601 chains at every 100th iterations."]},{"cell_type":"code","metadata":{"id":"Djg2C27eoLI7","colab_type":"code","colab":{}},"source":["\n","KL_divergence_with_SN = lambda mu_1, Sigma_1: 0.5*(np.trace(np.linalg.inv(Sigma_1)) + mu_1.dot(np.linalg.inv(Sigma_1)).dot(mu_1) - \n","                                           len(mu_1) + math.log(np.linalg.det(Sigma_1)))\n","\n","dim = 50\n","\n","\n","samples_matrix = np.zeros((601, 301, dim))\n","\n","\n","\n","for j in range(601):\n","\n","  # load the realised chains previously run\n","  chain = np.load('processed_BPS_data/processed_BPS_chain_' + str(j) + '_x_array.npz')\n","  single_sample = chain['x_array']\n","  \n","  # discard iterations after 30000:\n","  if not single_sample.shape == (301,dim):\n","    single_sample = single_sample[0:301,:]\n","  \n","  samples_matrix[j] = single_sample\n","  \n","  if j % 10 == 0:\n","    print('finishes loading {}th chains'.format(j))\n","\n","\n","# calculate empirical mean\n","means_array = np.mean(samples_matrix, axis = 0)\n","\n","# debugging\n","if not means_array.shape == (301,50):\n","  print('Code error: mean')\n","\n","\n","\n","# calculate empirical covariance matrix\n","samples_matrix_transpose = np.transpose(samples_matrix, (1,0,2))\n","covariance_mats_list = [np.cov(A, rowvar=False) for A in samples_matrix_transpose]\n","covariance_mats_array = np.array(covariance_mats_list)\n","\n","# debugging\n","if not covariance_mats_array.shape == (301,50,50):\n","  print('Code error: cov')\n","\n","\n","KL_div = np.zeros(301)\n","\n","# calculate KL divergence with standard normal of dimension 50\n","for i in range(301):\n","  KL_div[i] = KL_divergence_with_SN(means_array[i], covariance_mats_array[i])\n","\n","\n","\n","# save the KL divergence vector\n","path_to_save_KLs = 'processed_BPS_data/KL_divergences_BPS'\n","np.savez(path_to_save_KLs, KL_divergence = KL_div)\n"],"execution_count":null,"outputs":[]}]}