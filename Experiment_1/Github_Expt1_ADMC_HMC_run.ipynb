{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of experiment_1_ADMC_and_HMC.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM4HfI46xfLviPsw/QOHjH0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BRw07jlgjuhb","colab_type":"text"},"source":["# Before running the code"]},{"cell_type":"markdown","metadata":{"id":"Vx-zG8sSjCxL","colab_type":"text"},"source":["First, we need to install and import pints and other packages required. \n","\n","Then, we need to mount drive onto Colab and navigate to the right directories.\n"]},{"cell_type":"code","metadata":{"id":"iDMHbAq9u1t0","colab_type":"code","colab":{}},"source":["!pip install cma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwCP6-CG1WY3","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPA9b-0Gzxss","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/msc_dissertation/pints-master/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bp8aFY5-yS-U","colab_type":"code","colab":{}},"source":["!python setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"scVlCJ4CvPhN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math\n","import numpy.matlib as matlib\n","import operator\n","from time import perf_counter\n","import os\n","import os.path\n","\n","\n","import scipy\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import cma\n","import tabulate\n","import pints\n","import pints.toy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYGmD-BdD2oS","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/msc_dissertation/experiment_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4IbIx8YlT1_","colab_type":"text"},"source":["# Initialise starting points of MCMCs"]},{"cell_type":"markdown","metadata":{"id":"-GqpZFz8ldiY","colab_type":"text"},"source":["Initialise the dim-50 standard Gaussian distribution."]},{"cell_type":"code","metadata":{"id":"VpAxYecdlafz","colab_type":"code","colab":{}},"source":["dim = 50\n","mean = [0]*dim\n","var = [1]*dim\n","gaus_log_pdf = pints.toy.GaussianLogPDF(mean=mean, sigma=var)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jxHy7V2lkJg","colab_type":"text"},"source":["Initialise the starting points of Markov chains. These starting points are set to be the same across all samplers. Starting points are chosen such that they lie in a region of R^50 that have considerable probabilities. The following computation shows that the region [-3,3]^50 has a cumulative probability ~0.8735653. I am going to choose 100-600 starting points from this regions."]},{"cell_type":"code","metadata":{"id":"LK9eo6PMljdk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595356338054,"user_tz":-60,"elapsed":851,"user":{"displayName":"HKC Fok","photoUrl":"","userId":"18089383435240321925"}},"outputId":"b1b34bbc-abef-4c22-da0f-e2ddb16d2cab"},"source":["np.exp(dim*np.log(2*(scipy.stats.norm.cdf(3)-0.5)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8735653359931238"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"NkctqJNmlrki","colab_type":"text"},"source":["Because of limitation of time and computational power, 601 points are not enough for dispersing evenly across the region [-3.0,3.0]^50. This is a limitation of the experiment. 300 of all the starting points I choose are i \\times e_j for i = -3, -2, -1, 1, 2, 3 and j = 1, ..., 100 where e_j are standard unit vectors. The position of origin is also picked. The remaining 300 starting points are i \\times (e_j + e_{j+1}).\n","\n","According to Brooks et al, with good initial starting points, no burn-in period is needed for samplers without adaptation-free or warm-up period."]},{"cell_type":"code","metadata":{"id":"vzsrQ6_ul59Q","colab_type":"code","colab":{}},"source":["partial = np.identity(dim)\n","starting_pts = np.concatenate((np.array([[0]*dim]), 1*partial, 2*partial, 3*partial, \n","                               (-1)*partial, (-2)*partial, (-3)*partial), axis=0)\n","\n","another_half = np.concatenate((partial[1:], np.array([[1] + [0]*(dim-1)])), axis=0) + partial\n","\n","starting_pts = np.concatenate((starting_pts, another_half, 2*another_half, 3*another_half, \n","                               (-1)*another_half, (-2)*another_half, (-3)*another_half), axis=0)\n","\n","\n","starting_pts_list = list(starting_pts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cwif6wAlmBGJ","colab_type":"text"},"source":["This produces 601 starting points:"]},{"cell_type":"code","metadata":{"id":"xWHgxeHHmE5_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595409815857,"user_tz":-60,"elapsed":548,"user":{"displayName":"HKC Fok","photoUrl":"","userId":"18089383435240321925"}},"outputId":"5276309f-cee7-4d83-af61-541bb0e8efa8"},"source":["starting_pts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(601, 50)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"ef_PJ9XKmkcv","colab_type":"text"},"source":["# Run Haario-Bardenet on 50-dim Normal"]},{"cell_type":"markdown","metadata":{"id":"ZcwaF9yZkXEv","colab_type":"text"},"source":["First, this is the code that utilises MCMC.Controller to run the experiement. approximate_n_samples is the estimated upper threshold for the number of samples returned by ADMC in 2.5 minute.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l9gjXKI0kcVz","colab":{}},"source":["path_to_save_data = 'haario_bardenet_data/'\n","chain_filename = 'chain'\n","log_filename = 'logging'\n","mcmc_type = 'ADMC_'\n","n_hours = 5\n","\n","approximate_n_samples = 5000\n","\n","print('start running MCMC.')\n","for i in range(1):\n","  mcmc = pints.MCMCController(gaus_log_pdf, 1, [starting_pts_list[100*n_hours + 6*i + 59]], method=pints.HaarioBardenetACMC)\n","  \n","  mcmc.set_max_iterations(approximate_n_samples)\n","  mcmc.set_parallel(False)\n","  mcmc.set_initial_phase_iterations(1000)\n","  mcmc.set_chain_filename(path_to_save_data + mcmc_type + chain_filename + '_' + str(100*n_hours + 6*i + 59))\n","  mcmc.set_log_to_file(filename=path_to_save_data + mcmc_type + log_filename + '_' + str(100*n_hours + 6*i + 59), csv=True)\n","  mcmc.set_log_interval(iters=1000, warm_up=1100)\n","  mcmc.set_log_to_screen(True)\n","\n","  chains = mcmc.run()\n","  \n","  print('finishes the {}th chain'.format(str(100*n_hours + 6*i + 59)))\n","\n","print('finishes 1 hour of mcmc!')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CA3r0Uh-G9Ul","colab_type":"text"},"source":["# Run HMC on 50-dim Normal"]},{"cell_type":"code","metadata":{"id":"iFsJMemEkK9E","colab_type":"code","colab":{}},"source":["path_to_save_data = 'HMC_data/'\n","chain_filename = 'chain'\n","log_filename = 'logging'\n","mcmc_type = 'HMC_'\n","n_hours = 0\n","\n","approximate_n_samples = 100000\n","\n","print('start running MCMC.')\n","for i in range(20):\n","  mcmc = pints.MCMCController(gaus_log_pdf, 1, [starting_pts_list[100*n_hours + i]], \n","                              method=pints.HamiltonianMCMC)\n","  mcmc.set_max_iterations(approximate_n_samples)\n","  mcmc.set_parallel(False)\n","  mcmc.set_chain_filename(path_to_save_data + mcmc_type + 'chain_' + str(100*n_hours + i))\n","  mcmc.set_log_to_file(filename=path_to_save_data + mcmc_type + 'logging_' + \n","                       str(100*n_hours + i), csv=True)\n","  mcmc.set_log_interval(iters=50, warm_up=3)\n","  mcmc.set_log_to_screen(False)\n","\n","  chains = mcmc.run()\n","  \n","  j = 0\n","  for sampler in mcmc.samplers():\n","    np.savez(path_to_save_data + mcmc_type + 'chain_' + str(100*n_hours + i + j) + \n","             '_divergent_iters', divergent_iters = sampler.divergent_iterations())\n","    print('chain starting points: {}     divergent_iterations: {}'.format(100*n_hours + i + j, \n","                                                                          sampler.divergent_iterations()))\n","    j = j+1\n","\n","  print('finishes the {}th bunch of chains'.format(str(100*n_hours + i)))\n","\n","print('finishes the execution!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZ6EFmTZcyIm","colab_type":"text"},"source":["# Calculation of KL divergences at every 100 iterations (ADMC, HMC)"]},{"cell_type":"code","metadata":{"id":"AU4OHJ7bzuUQ","colab_type":"code","colab":{}},"source":["\n","KL_divergence_with_SN = lambda mu_1, Sigma_1: 0.5*(np.trace(np.linalg.inv(Sigma_1)) + mu_1.dot(np.linalg.inv(Sigma_1)).dot(mu_1) - \n","                                           len(mu_1) + math.log(np.linalg.det(Sigma_1)))\n","\n","dim = 50\n","\n","# iterations: from 0th to (num)th\n","num_iters_calculated = 28000\n","\n","\n","samples_matrix = np.zeros((601, num_iters_calculated + 1, dim))\n","\n","\n","\n","for j in range(601):\n","\n","  # load the realised chains previously run\n","  chain = np.genfromtxt('haario_bardenet_data_SFORD/ADMC_chain_' + str(j), delimiter = ',')\n","\n","  single_sample = chain[1:,:]\n","\n","  print('Dimension of {}th chain: {}'.format(j, single_sample.shape))\n","\n","\n","\n","  # revised: retain iterations from 0th to (num)th\n","  single_sample = single_sample[0:(num_iters_calculated + 1),:]\n","  \n","  samples_matrix[j] = single_sample\n","  \n","  if j % 10 == 0:\n","    print('finishes loading {}th chains'.format(j))\n","\n","\n","# calculate empirical mean\n","means_array = np.mean(samples_matrix, axis = 0)\n","\n","# debugging\n","if not means_array.shape == (num_iters_calculated + 1,50):\n","  print('Code error: mean')\n","\n","\n","\n","# calculate empirical covariance matrix\n","samples_matrix_transpose = np.transpose(samples_matrix, (1,0,2))\n","covariance_mats_list = [np.cov(A, rowvar=False) for A in samples_matrix_transpose]\n","covariance_mats_array = np.array(covariance_mats_list)\n","\n","# debugging\n","if not covariance_mats_array.shape == (num_iters_calculated + 1,50,50):\n","  print('Code error: cov')\n","\n","\n","KL_div = np.zeros(num_iters_calculated + 1)\n","\n","# calculate KL divergence with standard normal of dimension 50\n","for i in range(num_iters_calculated + 1):\n","  KL_div[i] = KL_divergence_with_SN(means_array[i], covariance_mats_array[i])\n","\n","\n","\n","# save the KL divergence vector\n","path_to_save_KLs = 'haario_bardenet_data_SFORD/KL_divergences_ADMC_' + str(num_iters_calculated)\n","np.savez(path_to_save_KLs, KL_divergence = KL_div)"],"execution_count":null,"outputs":[]}]}