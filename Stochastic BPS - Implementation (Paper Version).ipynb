{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[18-6-2020]\n",
    "\n",
    "Note to self: I may create a Github repository for all codes that I have written for this project. [?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[19-6-2020] \n",
    "\n",
    "Note to self: May make the code neater by trying to do something like: combining \"gaus_noise_presence\" and \"subsampling_noise_presence\" into \"nature_of_noise\", which can be tuned into \"artificial_gaussian_noise\" or \"subsampling_noise\".\n",
    "\n",
    "Any other actions to do in this aspect of characterising the noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy.matlib as matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest Level SBPS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[16-6-2020]\n",
    "\n",
    "Some of the steps that I have not implemented yet:\n",
    "1. Suspicious step: gradient ascent after acceptance/rejection step\n",
    "2. Introduction of auxiliary time\n",
    "\n",
    "\n",
    "Moreover, I may need to add more outputs to SBPS for purposes such as diagnosing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[19-6-2020]\n",
    "\n",
    "Catching an idea that I have:\n",
    "Because your project is not just about “your invention of new SBPS algorithm” –- there are other technical details to learn such as all those experiments you need to run, I wonder whether I should just go with the current original SBPS algorithm that I invented. [?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBPS(x0, v0, Time, lambda_ref, prob_dist, delta_t, k):\n",
    "    \n",
    "    # Need to ensure that euclidean norm of v0 equals 1.\n",
    "    \n",
    "    turn_pts = [x0]\n",
    "    list_of_velo = [v0]\n",
    "    striding_times = [0]\n",
    "    dim = x0.size\n",
    "    i = 1\n",
    "    x = x0\n",
    "    v = v0\n",
    "    t = 0\n",
    "    \n",
    "    delta_U_tilde, G_tilde, c_t_squared, log_grad_list_0 = eval_G_tilde_plus_emp_var(x, v, prob_dist)\n",
    "    next_array_of_obs = np.array([[0, G_tilde, c_t_squared]])    # time; G_tilde; variance\n",
    "    \n",
    "    \n",
    "    while t < Time:\n",
    "        \n",
    "        tau_bounce, observed_grad, next_array_of_obs = bounce_time_local_linear_reg_paper_version(x, v, next_array_of_obs, \n",
    "                                                                                                  prob_dist, delta_t, k)\n",
    "        beta = 1/lambda_ref\n",
    "        tau_ref = np.random.exponential(scale = beta)\n",
    "    \n",
    "    \n",
    "        tau = min(tau_bounce, tau_ref)\n",
    "        x = x + tau*v\n",
    "        t = t + tau\n",
    "        \n",
    "        \n",
    "        if tau_ref < tau_bounce:\n",
    "            unscaled_v = np.random.standard_normal(dim)\n",
    "            v = unscaled_v / np.linalg.norm(unscaled_v)\n",
    "        else:\n",
    "            v = reflection(v, observed_grad)\n",
    "            \n",
    "        \n",
    "        list_of_velo.append(v)\n",
    "        turn_pts.append(x)\n",
    "        striding_times.append(t)\n",
    "        i = i+1\n",
    "\n",
    "\n",
    "    return turn_pts, list_of_velo, striding_times     # [x_list, v_list, t_list]\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection of Velocity - Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(v, observed_grad):\n",
    "    \n",
    "    return v - 2*(np.sum(observed_grad*v)/(np.sum(observed_grad*observed_grad)))*observed_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Locations at arbitrary time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def x_v_t_arbitrary_times(turn_pts, list_of_velo, striding_times, intermediate_times):\n",
    "    \n",
    "    num_changes = len(turn_pts)\n",
    "    num_required_times = len(intermediate_times)\n",
    "    tiling_interm_times = np.transpose(np.tile(intermediate_times, (num_changes, 1)))\n",
    "    testing_mat_1 = tiling_interm_times - np.tile(striding_times, (num_required_times, 1))\n",
    "    testing_mat_2 = np.where(testing_mat_1 >= 0, 1, 0)\n",
    "    indices_no_later_than = np.sum(testing_mat_2, axis = 1) - 1\n",
    "        \n",
    "    turn_pts_no_later = [turn_pts[i] for i in indices_no_later_than]\n",
    "    velo_no_later = [list_of_velo[i] for i in indices_no_later_than]\n",
    "    stride_time_no_later = [striding_times[i] for i in indices_no_later_than]\n",
    "    \n",
    "       \n",
    "    interm_times = list(intermediate_times)\n",
    "        \n",
    "    list_of_coasting_times = list(map(operator.sub, interm_times, stride_time_no_later))\n",
    "    distance_strided = list(map(operator.mul, list_of_coasting_times, velo_no_later))\n",
    "    locations_at_required_times = list(map(operator.add, turn_pts_no_later, distance_strided))\n",
    "        \n",
    "\n",
    "    return locations_at_required_times, velo_no_later, interm_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating Cox Process: Local Linear Regression Upper Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another error discovered in Ari's algorithm: after a reflection of velocity, it seems that the empirical variance will change. Hence, I corrected it in this code. [?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bounce_time_local_linear_reg_paper_version(x, v, initialised_arr_of_obs, prob_dist, delta_t, k, \n",
    "                                               mu_0=0, sig_0=1, mu_1=0, sig_1=1):\n",
    "    \n",
    "    accepted = 0\n",
    "    time_coasted = 0\n",
    "    \n",
    "    \n",
    "    array_of_observations = initialised_arr_of_obs\n",
    "    \n",
    "    \n",
    "    while accepted == 0:\n",
    "        \n",
    "        hat_beta_0, hat_beta_1, Sigma_cov_mat = Bayesian_Linear_Regression_doub_gaus(array_of_observations, \n",
    "                                                                                 sig_0, mu_0, sig_1, mu_1)\n",
    "    \n",
    "        start_time = array_of_observations[-1,0]\n",
    "        extrapolated_var = array_of_observations[-1,2]\n",
    "    \n",
    "        proposed_time, upper_bound_intensity = adaptive_thinning_paper_version(start_time, hat_beta_0, hat_beta_1, \n",
    "                                                    Sigma_cov_mat, extrapolated_var, k, delta_t)\n",
    "    \n",
    "        time_coasted = proposed_time\n",
    "    \n",
    "        observed_grad, acc_rej_G, acc_reg_var, log_grad_list = eval_G_tilde_plus_emp_var(x + time_coasted*v, v, prob_dist)\n",
    "    \n",
    "        prob_of_acc = probability_of_acceptance(max(0, acc_rej_G), upper_bound_intensity)\n",
    "    \n",
    "        sampling_prob = np.random.uniform()\n",
    "    \n",
    "        if sampling_prob < prob_of_acc:\n",
    "            accepted = 1\n",
    "            \n",
    "            if prob_dist.nature_of_noise == 'artificial_gaus':\n",
    "                reinitialised_arr_of_obs = np.array([[0, -acc_rej_G, acc_reg_var]])\n",
    "                \n",
    "                \n",
    "            if prob_dist.nature_of_noise == 'subsampling':\n",
    "                \n",
    "                reflected_v = reflection(v, observed_grad)\n",
    "                reflected_dot_prods = log_grad_list.dot(reflected_v)\n",
    "                reflected_var = (prob_dist.num_obs**2)*(1 - prob_dist.n/prob_dist.num_obs)*np.var(reflected_dot_prods)\n",
    "                \n",
    "                reinitialised_arr_of_obs = np.array([[0, -acc_rej_G, reflected_var]])\n",
    "            \n",
    "        else:\n",
    "            new_obs = np.array([[time_coasted, acc_rej_G, acc_reg_var]])\n",
    "            array_of_observations = np.concatenate((array_of_observations, new_obs), axis=0)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    # What I need to return (may not be exhaustive): tau_bounce, observed_gradient, \n",
    "    # the re-initialised list of observations [[0,-G(t),c_t]].\n",
    "    \n",
    "    return time_coasted, observed_grad, reinitialised_arr_of_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function for calculating probability of acceptance, I adopt the rule of \"returning 0.5 if both \\tilde{G}(t) = 0 and lambda(t) = 0\". I wonder whether it is okay for me to write into the report that this case rarely occurs and the value I chose to output does not affect significantly the simulation?\n",
    "\n",
    "[# Key lies in: whether underflowing occurs frequently or rarely. [?]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def probability_of_acceptance(real_intensity, proposal_intensity):  # [G(t)]_{+}; lambda(t)\n",
    "    \n",
    "    if proposal_intensity > 0:\n",
    "        return min(1, real_intensity/proposal_intensity)\n",
    "    \n",
    "    elif proposal_intensity == 0 and real_intensity > 0:\n",
    "        return 1\n",
    "    \n",
    "    elif proposal_intensity == 0 and real_intensity == 0:\n",
    "        return 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for observation a noisy gradient and estimating the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: this function is applicable only when noise_presence is set to True!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_G_tilde_plus_emp_var(x, v, prob_dist):\n",
    "    \n",
    "    if prob_dist.nature_of_noise == 'artificial_gaus':\n",
    "        \n",
    "        delta_U_mean = prob_dist.ener_grad(x)\n",
    "        gaus_noise_vec = np.random.multivariate_normal(np.zeros(x.size), np.identity(x.size)*prob_dist.exact_gaus_noise_var)\n",
    "        \n",
    "        delta_U_tilde = delta_U_mean + gaus_noise_vec\n",
    "        # print('delta_U_tilde:', delta_U_tilde)\n",
    "        # print('v: ', v)\n",
    "        \n",
    "        G_tilde = v.dot(delta_U_tilde)\n",
    "        \n",
    "        c_t_squared = (v.dot(v))*prob_dist.exact_gaus_noise_var\n",
    "        \n",
    "        log_grad_list = ['N/A']\n",
    "        \n",
    "        \n",
    "    if prob_dist.nature_of_noise == 'subsampling':\n",
    "        \n",
    "        delta_U_tilde, log_grad_list = prob_dist.ener_grad(x, list_sampling = True)\n",
    "        dot_prod_list = log_grad_list.dot(v)\n",
    "        \n",
    "        G_tilde = v.dot(delta_U_tilde)\n",
    "        \n",
    "        c_t_squared = (prob_dist.num_obs**2)*(1 - prob_dist.n/prob_dist.num_obs)*np.var(dot_prod_list)\n",
    "        \n",
    "        \n",
    "    return delta_U_tilde, G_tilde, c_t_squared, log_grad_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N(sig_0, mu_0), N(sig_1, mu_1) are the priors I used. \n",
    "\n",
    "Each column of array_of_observations is [Time, G_tilde, variance].\n",
    "\n",
    "Side-note: my calculations of E[beta_0], E[beta_1], E[beta_1^2], E[beta_1*beta_2], E[beta_2^2] shows that none of them involves the normalisation constant p(G_1, ..., G_m). This makes me feel that if I use a Uniform distribution for beta_0, then these 5 expectations still have closed form. [?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Bayesian_Linear_Regression_doub_gaus(array_of_observations, sig_0, mu_0, sig_1, mu_1):\n",
    "    \n",
    "    # print(array_of_observations)\n",
    "    \n",
    "    list_of_times = array_of_observations[:,0]\n",
    "    list_of_G = array_of_observations[:,1]\n",
    "    list_of_variances = array_of_observations[:,2]\n",
    "    \n",
    "    S_1 = np.sum(1/list_of_variances)/2\n",
    "    S_2 = np.sum((1/list_of_variances)*(list_of_times))/2\n",
    "    S_3 = np.sum((1/list_of_variances)*(list_of_G))/2\n",
    "    \n",
    "    S_4 = np.sum((1/list_of_variances)*(list_of_times)*(list_of_G))/2\n",
    "    S_5 = np.sum((1/list_of_variances)*(list_of_times)*(list_of_times))/2\n",
    "    S_6 = np.sum((1/list_of_variances)*(list_of_G)*(list_of_G))/2\n",
    "    \n",
    "    \n",
    "    D_00 = -2*S_1 - 1/(sig_0**2)\n",
    "    D_01 = -2*S_2\n",
    "    D_02 = mu_0/(sig_0**2) + 2*S_3\n",
    "    \n",
    "    D_10 = -2*S_2\n",
    "    D_11 = -2*S_5 - 1/(sig_1**2)\n",
    "    D_12 = mu_1/(sig_1**2) + 2*S_4\n",
    "    \n",
    "    \n",
    "    if D_01 == 0:\n",
    "        \n",
    "        expect_beta0 = -D_02/D_00\n",
    "        expect_beta1 = -D_12/D_11\n",
    "        expect_beta0_squared = (D_02/D_00)**2 - 1/D_00\n",
    "        expect_beta1_squared = (D_12/D_11)**2 - 1/D_11\n",
    "        expect_beta0_beta1 = (D_02*D_12)/(D_00*D_11)\n",
    "    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        expect_beta0 = (D_01*D_12 - D_02*D_11)/(D_00*D_11 - D_01*D_10)\n",
    "        expect_beta1 = (D_10*D_02 - D_12*D_00)/(D_00*D_11 - D_01*D_10)\n",
    "        expect_beta0_squared = ((D_01*D_12 - D_02*D_11)/(D_00*D_11 - D_01*D_10))**2 + D_11/(D_01*D_10 - D_00*D_11)\n",
    "        expect_beta1_squared = ((D_10*D_02 - D_12*D_00)/(D_00*D_11 - D_01*D_10))**2 + D_00/(D_01*D_10 - D_00*D_11)\n",
    "        expect_beta0_beta1 = expect_beta0*expect_beta1 + D_10/(D_00*D_11 - D_01*D_10)\n",
    "    \n",
    "    \n",
    "    hat_beta_0 = expect_beta0\n",
    "    hat_beta_1 = expect_beta1\n",
    "    \n",
    "    Var_0 = expect_beta0_squared - (expect_beta0)**2\n",
    "    Var_1 = expect_beta1_squared - (expect_beta1)**2\n",
    "    Cov_01 = expect_beta0_beta1 - (expect_beta0)*(expect_beta1)\n",
    "    \n",
    "    Sigma_cov_mat = np.array([[Var_0, Cov_01], [Cov_01, Var_1]])\n",
    "    \n",
    "\n",
    "    \n",
    "    # need to calculate E[beta_0], E[beta_1], E[beta_1^2], E[beta_1*beta_2], E[beta_2^2]\n",
    "    \n",
    "    return hat_beta_0, hat_beta_1, Sigma_cov_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, I will also revisit the issue of flat prior (going through the Bayesian Lasso paper, or clarify with Ari Pakman), and then write another function for Bayesian linear regression using flat prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Bayesian_Linear_Regression_flat_prior(array_of_observations, sig_0, mu_0, sig_1, mu_1):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (The heart of controversies) Construct an Upper-Bound Intensity and Sample from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ari Pakman's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adaptive_thinning_paper_version(start_time, hat_beta_0, hat_beta_1, Sigma, extrapolated_var, k, delta_t):\n",
    "    \n",
    "    rho = lambda t: np.array([1,t]).dot(Sigma.dot(np.array([1,t]))) + extrapolated_var\n",
    "    gamma = lambda t: hat_beta_1*t + hat_beta_0 + k*np.sqrt(rho(t))\n",
    "    lambda_intensity = lambda t: max(0, gamma(t))\n",
    "    \n",
    "    \n",
    "    V = np.random.uniform()*(-1)+1\n",
    "    \n",
    "    total_intensity = -math.log(V)\n",
    "    num_piecewise_linear_travelled = 0\n",
    "    t = start_time\n",
    "    cumulative_intensity = 0\n",
    "    \n",
    "    num_loops = 0\n",
    "    \n",
    "    \n",
    "    while cumulative_intensity < total_intensity:\n",
    "        \n",
    "        num_piecewise_linear_travelled = num_piecewise_linear_travelled + 1\n",
    "        small_t = t\n",
    "        large_t = t + delta_t\n",
    "        \n",
    "        increment = delta_t*(lambda_intensity(small_t) + lambda_intensity(large_t))/2\n",
    "        cumulative_intensity = cumulative_intensity + increment\n",
    "        \n",
    "        t = t + delta_t\n",
    "        num_loops = num_loops + 1\n",
    "        \n",
    "        if num_loops % 100 == 0:\n",
    "            print('number of loops executed:', num_loops)\n",
    "        \n",
    "    remainder = total_intensity - (cumulative_intensity - increment)\n",
    "    slope = (lambda_intensity(large_t) - lambda_intensity(small_t))/(delta_t)\n",
    "    \n",
    "    quad_soln = quadratic_equation(slope, 2*lambda_intensity(small_t), -2*remainder)\n",
    "    \n",
    "    \n",
    "    if len(quad_soln[1]) == 1:\n",
    "        proposed_time = small_t + (quad_soln[1])[0]\n",
    "    elif len(quad_soln[1]) == 2:\n",
    "        list_soln = [x for x in quad_soln[1] if x > 0 and x <= delta_t]\n",
    "        proposed_time = small_t + list_soln[0]\n",
    "    \n",
    "    \n",
    "    return proposed_time, lambda_intensity(proposed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 % 2 ==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adaptive_thinning_original_version(hat_beta_0, hat_beta_1, extrapolated_var, Sigma, k, delta_t):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small function for solving quadratic equation ax^2 + bx +c = 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quadratic_equation(a,b,c):\n",
    "    \n",
    "    if a==0 and b==0 and c==0:\n",
    "        return ['a = b = c = 0','nannannan']\n",
    "    \n",
    "    elif a==0 and b==0 and (not c==0):\n",
    "        return ['a = b = 0 but not c = 0', 'nannannan']\n",
    "    \n",
    "    elif a==0 and (not b==0):\n",
    "        return ['proper linear equation', [-c/b]]\n",
    "    \n",
    "    elif (not a == 0) and (b**2 - 4*a*c < 0):\n",
    "        return ['complex roots', 'nannannan']\n",
    "    \n",
    "    elif (not a == 0) and (b**2 - 4*a*c == 0):\n",
    "        return ['repeated real root', [-b/(2*a)]]\n",
    "    \n",
    "    elif (not a == 0) and (b**2 - 4*a*c > 0):\n",
    "        return ['distinct real roots', [(-b - math.sqrt(b**2 - 4*a*c))/(2*a), (-b + math.sqrt(b**2 - 4*a*c))/(2*a)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the following distributions, there are 2 types of noises: artificial gaussian noise and subsampling noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Gaussian (from my BPS toy codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me: don't know how to add noise for energy function, but feels that this will not be used.\n",
    "\n",
    "Notice: note that this noisy version of multivariate gaussian is probably too simple --- the noise variable added to the energy gradient anywhere has the same probability distribution. More complicated examples may have to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gaussian:\n",
    "    \n",
    "    def __init__(self, mean, cov_mat, noise_presence, exact_gaus_noise_var, nature_of_noise = 'artificial_gaus'):\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.cov_mat = cov_mat\n",
    "        self.dim = mean.size\n",
    "        \n",
    "        \n",
    "        self.noise_presence = noise_presence\n",
    "        self.exact_gaus_noise_var = exact_gaus_noise_var\n",
    "        self.nature_of_noise = 'artificial_gaus'\n",
    "    \n",
    "    \n",
    "    def energy(self, x): # constant coeff is discarded\n",
    "\n",
    "        return 0.5*((x - self.mean).dot(np.linalg.inv(self.cov_mat)).dot(x - self.mean)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def ener_grad(self, x):\n",
    "\n",
    "        return (np.linalg.inv(self.cov_mat)).dot(x - self.mean)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distribution: Noisy Access to Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Dataset (Observations). Subsampling (without replacement) is performed to calculate gradient.\n",
    "\n",
    "Me: so do I have exact access to the energy function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Likelihood and N(0_d, I_d) used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Logistic_Regression_Bern_likelihood:\n",
    "    \n",
    "    \n",
    "    def __init__(self, obs_input, obs_output, noise_presence, subsampling_num,\n",
    "                 nature_of_noise = 'subsampling'):   # obs_input: [N,d]; obs_output: [N]; subsampling_num: n\n",
    "\n",
    "        self.num_obs = obs_input.shape[0]\n",
    "        self.obs_dim = obs_input.shape[1]\n",
    "        self.obs_input = obs_input\n",
    "        self.obs_output = obs_output\n",
    "        \n",
    "        self.n = subsampling_num\n",
    "        self.noise_presence = noise_presence\n",
    "        self.nature_of_noise = nature_of_noise\n",
    "        \n",
    "        self.prior_dist = Gaussian(mean = np.zeros(obs_input.shape[1]), cov_mat = np.identity(obs_input.shape[1]), \n",
    "                                   noise_presence = False, exact_gaus_noise_var = 0.01, nature_of_noise = 'artificial_gaus')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def energy(self, w, var_calc):  # w: [d]\n",
    "        \n",
    "        if self.noise_presence == True:\n",
    "            subsamples_index = np.random.choice(self.num_obs, self.n, replace=False)\n",
    "            subsamples_input = self.obs_input[subsamples_index]\n",
    "            subsamples_output = self.obs_output[subsamples_index]\n",
    "        \n",
    "        if self.noise_presence == False:\n",
    "            subsamples_input = self.obs_input\n",
    "            subsamples_output = self.obs_output\n",
    "        \n",
    "        \n",
    "        interm_1 = (subsamples_input).dot(w)\n",
    "        interm_11 = np.log(1 / (1 + np.exp(-interm_1)))\n",
    "        interm_12 = np.log(1 - (1/(1 + np.exp(-interm_1))))\n",
    "        \n",
    "        indiv_likeli_ener = subsamples_output*(interm_11) + (1 - subsamples_output)*(interm_12)\n",
    "        \n",
    "        if self.subsampling_noise_presence == True:\n",
    "            return self.prior_dist.energy(w) - (self.num_obs/self.n)*np.sum(indiv_likeli_ener)\n",
    "        \n",
    "        if self.subsampling_noise_presence == False:\n",
    "            return self.prior_dist.energy(w) - np.sum(indiv_likeli_ener)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def ener_grad(self, w, list_sampling): # x: np.array [d]\n",
    "        \n",
    "        if self.noise_presence == True:\n",
    "            subsamples_index = np.random.choice(self.num_obs, self.n, replace=False)\n",
    "            subsamples_input = self.obs_input[subsamples_index]\n",
    "            subsamples_output = self.obs_output[subsamples_index]\n",
    "\n",
    "            \n",
    "        if self.noise_presence == False:\n",
    "            subsamples_input = self.obs_input\n",
    "            subsamples_output = self.obs_output\n",
    "        \n",
    "        \n",
    "        lin_trans = (subsamples_input).dot(w)\n",
    "        logistic_lin_trans = subsamples_output - 1/(1 + np.exp(-lin_trans))\n",
    "        sum_to_R = logistic_lin_trans.dot(subsamples_input)\n",
    "        \n",
    "        sampled_grads = np.transpose(np.tile(logistic_lin_trans,(self.obs_dim, 1)))*subsamples_input\n",
    "        # if noise_presence = False, then sampled_grads is the full N-by-d array of gradient the log-likelihood\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.noise_presence == True and list_sampling == True:\n",
    "            return self.prior_dist.ener_grad(w) - (self.num_obs/self.n)*sum_to_R, sampled_grads\n",
    "        \n",
    "        if self.noise_presence == True and list_sampling == False:\n",
    "            return self.prior_dist.ener_grad(w) - (self.num_obs/self.n)*sum_to_R\n",
    "        \n",
    "        if self.subsampling_noise_presence == False:\n",
    "            return self.prior_dist.ener_grad(w) - sum_to_R\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One more toy distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me: perhaps can do another (common and/or well-understood?) multivariate distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging my codes by running them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try all these with Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.zeros(2)\n",
    "cov_mat = np.identity(2)\n",
    "\n",
    "stand_gaus = Gaussian(mean = mean, cov_mat = cov_mat, noise_presence = True, \n",
    "                      exact_gaus_noise_var=0.01, nature_of_noise = 'artificial_gaus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the function is stuck in adaptive_thinning_paper_version. This might be due to the proposal intensity gets negative as t tends to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exceptionally weird result. When I run SBPS, 26, 79, 57 bounces and refreshment occurs. Using the same x0, v0, Time, lambda_ref for the basic BPS, 150, and more than 200 bounces and refreshment occurs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting everything together\n",
    "\n",
    "gaus_location = np.transpose(x_list)\n",
    "\n",
    "horizontal = gaus_location[0]\n",
    "vertical = gaus_location[1]\n",
    "\n",
    "colors = (0,0,0)\n",
    "area = (np.pi*10)\n",
    "num_points = len(horizontal)\n",
    "\n",
    "# Plot\n",
    "\n",
    "# plt.imshow(ener_vals_gaus, cmap='YlOrRd', extent=[-3, 3, -3, 3])\n",
    "\n",
    "# plt.yticks(np.arange(-3, 4, 1))\n",
    "# plt.xticks(np.arange(-3, 4, 1))\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "plt.scatter(horizontal, vertical, s=area/2, c = np.array([[0,0,0]]))    # if this example code is correct, 1st arg is x-axis; 2nd arg is y-axis.\n",
    "plt.title('Running SBPS on Standard Gaussian with dim=4')\n",
    "\n",
    "for i in range(0,5):\n",
    "    plt.arrow(horizontal[i], vertical[i], horizontal[i+1]-horizontal[i], vertical[i+1]-vertical[i], width=0.0001, length_includes_head=True, head_width=0.0001, shape='full')\n",
    "\n",
    "\n",
    "plt.annotate('start', (0, 0))\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: it seems to me that both BPS and SBPS will run into this seemingly problematic circular shape of bouncing (if I initialise x0 = (-100,-100) and v0 = (1,1).)--- probably because lambda_ref is set to be small, and exact_gaus_noise_var is also set small, which does not randomising bounce enough. [?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then, try with the Logistic Regression problem with Bernoulli likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_input = np.random.normal(0, 5, (100,4))\n",
    "test_output = np.random.binomial(1, 0.5, 100)\n",
    "\n",
    "test_dist_logistic_1 = Logistic_Regression_Bern_likelihood(obs_input = test_input, obs_output = test_output, \n",
    "                                                           noise_presence = True, subsampling_num = 10, \n",
    "                                                           nature_of_noise = 'subsampling')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_list, v_list, t_list = SBPS(x0 = np.zeros(4), v0 = np.array([1,1,1,1]), Time = 20, lambda_ref = 0.01, \n",
    "                              prob_dist = test_dist_logistic_1, delta_t = 0.1, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.000000000000025"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_list[1122].dot(v_list[1122])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very weird: in a time period of 20, 1122 bounces occurs! And note that none of them is \"refreshment\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with a more well-constructed logistic regression scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fix a true parameter w in R^20. Then, 1000 independent inputs are generated from multivariate standard Gaussian. For each input x_i in R^20, we sample the output label y_i from Bernoulli(1/(1 + exp(-w.dot(x_i)))).\n",
    "\n",
    "This construction of dataset is nearly the same as the one in experiment 7.1 of SBPS paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = np.random.randint(-5, 6, 20)\n",
    "obs_input = np.random.multivariate_normal(np.zeros(20), np.identity(20), 1000)\n",
    "\n",
    "prob_params = 1/(1 + np.exp(-obs_input.dot(w)))\n",
    "\n",
    "obs_output = np.random.binomial(1, prob_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dist_logistic_2 = Logistic_Regression_Bern_likelihood(obs_input = obs_input, obs_output = obs_output, \n",
    "                                                           noise_presence = True, subsampling_num = 100, \n",
    "                                                           nature_of_noise = 'subsampling')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_list_2, v_list_2, t_list_2 = SBPS(x0 = np.zeros(20), v0 = 0.5*np.ones(20), Time = 20, lambda_ref = 0.01, \n",
    "                              prob_dist = test_dist_logistic_2, delta_t = 0.1, k = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of x_list_2:  82\n",
      "length of v_list_2:  82\n",
      "length of t_list_2:  82\n"
     ]
    }
   ],
   "source": [
    "print('length of x_list_2: ', len(x_list_2))\n",
    "print('length of v_list_2: ', len(v_list_2))\n",
    "print('length of t_list_2: ', len(t_list_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command verifies that the magnitude of the velocity vector stays the same after bounces.\n",
    "\n",
    "The test is passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
