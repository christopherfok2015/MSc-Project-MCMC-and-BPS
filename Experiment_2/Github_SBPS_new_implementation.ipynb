{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Github_SBPS_new_implementation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOl9tV757In9z0Yo9CLnHoU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Zgs5lhQMTid-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math\n","import numpy.matlib as matlib\n","import matplotlib.pyplot as plt\n","import operator\n","\n","from time import perf_counter\n","import scipy.stats"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AW5PfwDe4Vkg","colab_type":"text"},"source":["Note:\n","\n","In the following, when the name of a function, an array, etc. contains the word 'tilde', noise is involved in some way."]},{"cell_type":"markdown","metadata":{"id":"uUJiMHY4ZQCi","colab_type":"text"},"source":["# Function: SBPS MCMC"]},{"cell_type":"markdown","metadata":{"id":"QoZwDZ7a-zTF","colab_type":"text"},"source":["This is the function we use to run SBPS MCMC. It will output:\n","1. the turning points of the trajectory\n","2. velocities there at those points\n","3. times there at those points\n","4. total cpu seconds used there\n","5. total log-pdf evaluations used there"]},{"cell_type":"code","metadata":{"id":"QJ0WyD0XT1YM","colab_type":"code","colab":{}},"source":["# This is the function we call the SBPS MCMC.\n","# It will output the turning points of the trajectory, velocities, times there,\n","# cpu seconds used there, and number of pdf evaluations used there.\n","\n","\n","\n","# num_lin_reg is the hyper-param for num of points used to do linear regression.\n","def SBPS(x0, v0, Time, lambda_ref, prob_dist, num_lin_reg, k, d_geom):\n","    \n","    cpu_time_executed = perf_counter()\n","    computational_times = [0]\n","    start_time = cpu_time_executed\n","    \n","    turn_pts = [x0]\n","    list_of_velo = [v0]\n","    striding_times = [0]\n","    total_evals_list = [1]\n","    dim = x0.size\n","    i = 1\n","    x = x0\n","    v = v0\n","    t = 0\n","    total_evals = 1\n","    \n","    next_singleton_ob_delta_U, G_tilde, c_t_squared, next_singleton_ob_log_grad_list =\\\n","    eval_G_tilde_plus_emp_var(x, v, prob_dist)\n","    next_array_of_singleton_ob = np.array([[0, G_tilde, c_t_squared]])    # time; G_tilde; variance\n","    \n","    \n","    while t < Time:\n","        # Next bounce\n","        # next_array_of_obs is a np.array being [0, G_t, variance]\n","        tau_bounce, observed_grad, next_array_of_singleton_ob, next_singleton_ob_delta_U,\\\n","        next_singleton_ob_log_grad_list, num_evals =\\\n","        New_simulation_Cox_Process(x, v, next_array_of_singleton_ob, next_singleton_ob_delta_U, \n","                                   next_singleton_ob_log_grad_list, prob_dist, num_lin_reg, k, d_geom)\n","        \n","        total_evals = total_evals + num_evals\n","        \n","        # Next refreshment\n","        beta = 1/lambda_ref\n","        tau_ref = np.random.exponential(scale = beta)\n","    \n","    \n","        tau = min(tau_bounce, tau_ref)\n","        x = x + tau*v\n","        t = t + tau\n","        \n","        \n","        if tau_ref < tau_bounce:\n","            v = np.random.standard_normal(dim)\n","\n","        else:\n","            v = reflection(v, observed_grad)\n","            \n","        \n","        cpu_time_executed = perf_counter() - start_time\n","        \n","        list_of_velo.append(v)\n","        turn_pts.append(x)\n","        striding_times.append(t)\n","        computational_times.append(cpu_time_executed)\n","        total_evals_list.append(total_evals)\n","\n","        if i%50 == 0:\n","          print('Complete {}th change of velocity'.format(i))\n","        i = i+1\n","\n","\n","    return turn_pts, list_of_velo, striding_times, total_evals_list, computational_times\n","    # x_list, v_list, t_list, evals_list, cpu_time_list\n","       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nO1bDmqOaAiV","colab_type":"text"},"source":["# Function: reflection of velocity"]},{"cell_type":"markdown","metadata":{"id":"6wHj5KNp_Zqy","colab_type":"text"},"source":["v is reflected according to a noisy gradient at some position."]},{"cell_type":"code","metadata":{"id":"Oisi36eEZOEd","colab_type":"code","colab":{}},"source":["\n","def reflection(v, observed_grad):\n","    \n","    return v - 2*(np.sum(observed_grad*v)/(np.sum(observed_grad*observed_grad)))*observed_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FI_IN8dkaaR4","colab_type":"text"},"source":["# Function: x_v_t_arbitrary_times"]},{"cell_type":"markdown","metadata":{"id":"B8TwO7dK-l-3","colab_type":"text"},"source":["This can compute the position and velocity of a particle at any time points along the trajectory."]},{"cell_type":"code","metadata":{"id":"APl-5WoQa5Wq","colab_type":"code","colab":{}},"source":["\n","def x_v_t_arbitrary_times(turn_pts, list_of_velo, striding_times, intermediate_times):\n","    \n","    num_changes = len(turn_pts)\n","    num_required_times = len(intermediate_times)\n","    tiling_interm_times = np.transpose(np.tile(intermediate_times, (num_changes, 1)))\n","    testing_mat_1 = tiling_interm_times - np.tile(striding_times, (num_required_times, 1))\n","    testing_mat_2 = np.where(testing_mat_1 >= 0, 1, 0)\n","    indices_no_later_than = np.sum(testing_mat_2, axis = 1) - 1\n","        \n","    turn_pts_no_later = [turn_pts[i] for i in indices_no_later_than]\n","    velo_no_later = [list_of_velo[i] for i in indices_no_later_than]\n","    stride_time_no_later = [striding_times[i] for i in indices_no_later_than]\n","    \n","       \n","    interm_times = list(intermediate_times)\n","        \n","    list_of_coasting_times = list(map(operator.sub, interm_times, stride_time_no_later))\n","    distance_strided = list(map(operator.mul, list_of_coasting_times, velo_no_later))\n","    locations_at_required_times = list(map(operator.add, turn_pts_no_later, distance_strided))\n","        \n","\n","    return locations_at_required_times, velo_no_later, interm_times\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGs1ehZsfS0M","colab_type":"text"},"source":["# Function: Probability of Acceptance"]},{"cell_type":"markdown","metadata":{"id":"TM-ph-6p-TvW","colab_type":"text"},"source":["A acceptance-rejection step is present in bounce-time simulation.\n","\n","This function returns simply the probability of acceptance.\n","\n","It aims to tackle the division-by-zero issue.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_YQv65k-fWuV","colab_type":"code","colab":{}},"source":["\n","def probability_of_acceptance(real_intensity, proposal_intensity):  # [G(t)]_{+}; lambda(t)\n","    \n","    if proposal_intensity > 0:\n","        return min(1, real_intensity/proposal_intensity)\n","    \n","    elif proposal_intensity == 0 and real_intensity > 0:\n","        return 1\n","    \n","    elif proposal_intensity == 0 and real_intensity == 0:\n","        return 0.5\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71iYpJ4V9TRx","colab_type":"text"},"source":["# Function: eval_G_tilde_plus_emp_var"]},{"cell_type":"markdown","metadata":{"id":"JK6Zo7ne-Kch","colab_type":"text"},"source":["Given x, v, this function outputs the following:\n","\n","1. the noisy energy gradient evaluated at x,\n","2. the noisy G value: (ener grad) dot product with (velocity)\n","3. the empirical variance of the G value\n","4. A list of gradient of log(something). See Equation (10).\n"]},{"cell_type":"code","metadata":{"id":"rckEVDgW9Vlc","colab_type":"code","colab":{}},"source":["def eval_G_tilde_plus_emp_var(x, v, prob_dist):\n","    \n","    if prob_dist.nature_of_noise == 'artificial_gaus':\n","        \n","        delta_U_mean = prob_dist.ener_grad(x)\n","        gaus_noise_vec = np.random.multivariate_normal(np.zeros(x.size), np.identity(x.size)*prob_dist.exact_gaus_noise_var)\n","        \n","        delta_U_tilde = delta_U_mean + gaus_noise_vec\n","        # print('delta_U_tilde:', delta_U_tilde)\n","        # print('v: ', v)\n","        \n","        G_tilde = v.dot(delta_U_tilde)\n","        \n","        c_t_squared = (v.dot(v))*prob_dist.exact_gaus_noise_var\n","        \n","        log_grad_list = 'N/A'\n","        \n","        \n","    if prob_dist.nature_of_noise == 'subsampling':\n","        \n","        # log_grad is not the energy gradient; Equation (10) of the paper is followed.\n","        delta_U_tilde, log_grad_list = prob_dist.ener_grad(x, list_sampling = True)\n","        dot_prod_list = log_grad_list.dot(v)\n","        \n","        G_tilde = v.dot(delta_U_tilde)\n","        \n","        c_t_squared = (prob_dist.num_obs**2)*(1 - prob_dist.n/prob_dist.num_obs)*np.var(dot_prod_list)\n","        \n","\n","    return delta_U_tilde, G_tilde, c_t_squared, log_grad_list\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr2uCcgLK9zI","colab_type":"text"},"source":["# Function: Bayesian Linear Regression - Approach 1 (assuming Gaussian Priors on beta0, beta1)"]},{"cell_type":"markdown","metadata":{"id":"6VzPWicrLVuF","colab_type":"text"},"source":["The following calculates the mean and covariance matrix of (beta_0, beta_1) of the posterior distribution,\n","\n","after observations of G_tildes at different times are given."]},{"cell_type":"code","metadata":{"id":"vkXi93u1K_17","colab_type":"code","colab":{}},"source":["\n","def Bayesian_Linear_Regression_doub_gaus(array_of_observations, sig_0, mu_0, sig_1, mu_1):\n","    \n","    # print(array_of_observations)\n","    \n","    array_of_times = array_of_observations[:,0]\n","    array_of_Gs = array_of_observations[:,1]\n","    array_of_variances = array_of_observations[:,2]\n","    \n","    S_1 = np.sum(1/array_of_variances)/2\n","    S_2 = np.sum((1/array_of_variances)*(array_of_times))/2\n","    S_3 = np.sum((1/array_of_variances)*(array_of_Gs))/2\n","    \n","    S_4 = np.sum((1/array_of_variances)*(array_of_times)*(array_of_Gs))/2\n","    S_5 = np.sum((1/array_of_variances)*(array_of_times)*(array_of_times))/2\n","    S_6 = np.sum((1/array_of_variances)*(array_of_Gs)*(array_of_Gs))/2\n","    \n","    \n","    D_00 = -2*S_1 - 1/(sig_0**2)\n","    D_01 = -2*S_2\n","    D_02 = mu_0/(sig_0**2) + 2*S_3\n","    \n","    D_10 = -2*S_2\n","    D_11 = -2*S_5 - 1/(sig_1**2)\n","    D_12 = mu_1/(sig_1**2) + 2*S_4\n","    \n","    \n","    if D_01 == 0:\n","        \n","        expect_beta0 = -D_02/D_00\n","        expect_beta1 = -D_12/D_11\n","        expect_beta0_squared = (D_02/D_00)**2 - 1/D_00\n","        expect_beta1_squared = (D_12/D_11)**2 - 1/D_11\n","        expect_beta0_beta1 = (D_02*D_12)/(D_00*D_11)\n","    \n","    \n","    \n","    else:\n","        expect_beta0 = (D_01*D_12 - D_02*D_11)/(D_00*D_11 - D_01*D_10)\n","        expect_beta1 = (D_10*D_02 - D_12*D_00)/(D_00*D_11 - D_01*D_10)\n","        expect_beta0_squared = ((D_01*D_12 - D_02*D_11)/(D_00*D_11 - D_01*D_10))**2 + D_11/(D_01*D_10 - D_00*D_11)\n","        expect_beta1_squared = ((D_10*D_02 - D_12*D_00)/(D_00*D_11 - D_01*D_10))**2 + D_00/(D_01*D_10 - D_00*D_11)\n","        expect_beta0_beta1 = expect_beta0*expect_beta1 + D_10/(D_00*D_11 - D_01*D_10)\n","    \n","    \n","    hat_beta_0 = expect_beta0\n","    hat_beta_1 = expect_beta1\n","    \n","    Var_0 = expect_beta0_squared - (expect_beta0)**2\n","    Var_1 = expect_beta1_squared - (expect_beta1)**2\n","    Cov_01 = expect_beta0_beta1 - (expect_beta0)*(expect_beta1)\n","    \n","    Sigma_cov_mat = np.array([[Var_0, Cov_01], [Cov_01, Var_1]])\n","    \n","\n","    \n","    # need to calculate E[beta_0], E[beta_1], E[beta_1^2], E[beta_1*beta_2], E[beta_2^2]\n","    \n","    return hat_beta_0, hat_beta_1, Sigma_cov_mat\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MEhHTZFXsyTo","colab_type":"text"},"source":["# Function: Bayesian Linear Regression - Approach 2 (center G_tildes without centering times)"]},{"cell_type":"markdown","metadata":{"id":"Sj8l4dJGtLL7","colab_type":"text"},"source":["In this approach, hat_beta_0 is taken to be the mean of G_tilde's. Only beta_1 (but not beta_0) is modelled as a random variable.\n","\n","The major difference between this and the Bayesian Lasso paper's one is that this model does not center the times. I predict that the latter will perform better than this.\n","\n","See pp. ??-?? of my report for more details."]},{"cell_type":"code","metadata":{"id":"i1yoF1fis8YQ","colab_type":"code","colab":{}},"source":["def Bayesian_Linear_Regression_Pakman(array_of_observations, sig_1, mu_1):\n","  # columns of array_of_observations\n","  array_of_times = array_of_observations[:,0]\n","  array_of_ys = array_of_observations[:,1]\n","  array_of_vars = array_of_observations[:,2]\n","\n","  hat_beta_0 = np.mean(array_of_ys)\n","\n","\n","  P = 0.5/(sig_1**2) + 0.5*np.sum(array_of_times*array_of_times/array_of_vars)\n","  Q = np.sum(array_of_times*(hat_beta_0 - array_of_ys)/array_of_vars) - mu_1/(sig_1**2)\n","\n","\n","  hat_beta_1 = -0.5*Q/P\n","  post_cov_11 = 0.5/P\n","\n","  Sigma_cov_mat = np.array([[0,0],[0,post_cov_11]])\n","\n","\n","  return hat_beta_0, hat_beta_1, Sigma_cov_mat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B32fHzSEkgbr","colab_type":"text"},"source":["# Function: Bayesian Linear Regression - Approach 3 (following the Bayesian-Lasso paper)"]},{"cell_type":"markdown","metadata":{"id":"hcDpcAqNksxS","colab_type":"text"},"source":["In this approach, the time points [t_1, ..., t_m] are first standardised before the linear regression model is applied. See my report pp.??-?? for more details."]},{"cell_type":"code","metadata":{"id":"iul2UiyPmQ1-","colab_type":"code","colab":{}},"source":["def Bayesian_Linear_Regression_BayesLasso(array_of_observations, sig_1, mu_1):\n","  \n","  # standardising the time points\n","  array_of_times = array_of_observations[:,0]\n","  mean_of_times = np.mean(array_of_times)\n","  SD_of_times = np.sqrt(float(np.cov(array_of_times)))\n","\n","\n","  # mean of G_tilde's\n","  array_of_ys = array_of_observations[:,1]\n","  mean_of_ys = np.mean(array_of_ys)\n","\n","\n","  # array of empirical variance\n","  array_of_vars = array_of_observations[:,2]\n","\n","\n","  k_vec = (array_of_times - mean_of_times)/SD_of_times\n","  l_vec = mean_of_ys - array_of_ys\n","\n","\n","  P_tilde = 0.5/(sig_1**2) + 0.5*np.sum(k_vec*k_vec/array_of_vars)\n","  Q_tilde = np.sum(k_vec*l_vec/array_of_vars) - mu_1/(sig_1**2)\n","\n","\n","  hat_beta_1 = -0.5*Q_tilde/(SD_of_times*P_tilde)\n","  hat_beta_0 = mean_of_ys - hat_beta_1*mean_of_times\n","  post_cov_11 = 0.5/((SD_of_times**2)*P_tilde)\n","\n","  Sigma_cov_mat = np.array([[0,0],[0,post_cov_11]])\n","\n","\n","  return hat_beta_0, hat_beta_1, Sigma_cov_mat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kaNxLv60sn9J","colab_type":"text"},"source":["# Function: new_simulation_Cox_Process"]},{"cell_type":"code","metadata":{"id":"unrMSKN7sy87","colab_type":"code","colab":{}},"source":["\n","def New_simulation_Cox_Process(x, v, array_of_starting_singleton_ob, starting_singleton_delta_U, \n","                               starting_singleton_log_grad_list, prob_dist, num_lin_reg, k, d_geom):\n","\n","  dim = len(x)\n","  current_location = x\n","  \n","  termination = 0\n","  time_coasted = 0\n","  initialised_array_of_singleton_ob = array_of_starting_singleton_ob\n","  initialised_singleton_delta_U = starting_singleton_delta_U\n","  initialised_singleton_log_grad_list = starting_singleton_log_grad_list\n","\n","  num_evals = 0\n","  t_geom = d_geom/np.linalg.norm(v)\n","\n","  normalised_v = v/np.linalg.norm(v)\n","  tiling_v = np.tile(normalised_v, (num_lin_reg - 1, 1))\n","  mult_factors = np.transpose(np.tile(np.arange(1,num_lin_reg)*d_geom/(num_lin_reg-1), (dim,1)))\n","  moving_points_of_reg_obs = tiling_v*mult_factors\n","\n","  moving_times_for_reg = np.arange(1,num_lin_reg)*d_geom/((num_lin_reg-1)*np.linalg.norm(v))\n","\n","  # This counts the number of while loops performed in the following.\n","  regression_index = 0\n","\n","  sig_0 = 1\n","  sig_1 = 1\n","  mu_0 = 0\n","  mu_1 = 0\n","\n","  while termination == 0:\n","    # test whether the SBPS MCMC is stuck here\n","    # investigate why setting d_geom to low value is problematic.\n","    if regression_index % 100 == 99:\n","      print('Now, running the {}th segment of simulation of Cox Process'.format(regression_index))\n","      print('hat_beta_0: {}'.format(hat_beta_0))\n","      print('hat_beta_1: {}'.format(hat_beta_1))\n","      print('Sigma_cov_mat: {}'.format(Sigma_cov_mat))\n","    # Step 1: Bayesian Linear Regression\n","\n","    points_of_regression_obs = current_location + moving_points_of_reg_obs\n","    \n","    array_for_lin_reg = initialised_array_of_singleton_ob      # 2D array of dimension (1,3)\n","\n","    lin_reg_delta_U_list = [initialised_singleton_delta_U]\n","\n","    if prob_dist.nature_of_noise == 'artificial_gaus':\n","      \n","      for i in range(num_lin_reg-1):\n","        BAYES_delta_U_tilde, BAYES_G_tilde, BAYES_c_t_squared, BAYES_log_grad_list =\\\n","        eval_G_tilde_plus_emp_var(points_of_regression_obs[i], v, prob_dist)\n","        arr_observation = np.array([[moving_times_for_reg[i], BAYES_G_tilde, BAYES_c_t_squared]])\n","        \n","        array_for_lin_reg = np.concatenate((array_for_lin_reg, arr_observation), axis = 0)\n","\n","        lin_reg_delta_U_list.append(BAYES_delta_U_tilde)\n","\n","        # count the number of pdf evals\n","        num_evals = num_evals + 1\n","\n","    if prob_dist.nature_of_noise == 'subsampling':\n","      \n","      sequence_of_log_grad_lists = [initialised_singleton_log_grad_list]\n","\n","      for i in range(num_lin_reg-1):\n","        BAYES_delta_U_tilde, BAYES_G_tilde, BAYES_c_t_squared, BAYES_log_grad_list =\\\n","        eval_G_tilde_plus_emp_var(points_of_regression_obs[i], v, prob_dist)\n","        arr_observation = np.array([[moving_times_for_reg[i], BAYES_G_tilde, BAYES_c_t_squared]])\n","        \n","        array_for_lin_reg = np.concatenate((array_for_lin_reg, arr_observation), axis = 0)\n","        \n","        lin_reg_delta_U_list.append(BAYES_delta_U_tilde)\n","\n","        sequence_of_log_grad_lists.append(BAYES_log_grad_list)\n","\n","        # count the number of pdf evals\n","        num_evals = num_evals + 1\n","    \n","    hat_beta_0, hat_beta_1, Sigma_cov_mat =\\\n","    Bayesian_Linear_Regression_doub_gaus(array_for_lin_reg, sig_0, mu_0, sig_1, mu_1)\n","    \n","\n","\n","    # Step 2: Propose events from this upper NHPP. Perform acceptance-rejection\n","    # The first arrival of an NHPP with time duration [0, d_geom/|v|] is simulated.\n","    \n","    if prob_dist.nature_of_noise == 'artificial_gaus':\n","      event_time, delta_U_tilde, G_tilde, c_t_squared, log_grad_list, num_evals_ADAPTIVE =\\\n","      analytic_Bi_adaptive_thinning(current_location, v, array_for_lin_reg, lin_reg_delta_U_list, \n","                                    hat_beta_0, hat_beta_1, Sigma_cov_mat, prob_dist, \n","                                    num_lin_reg, k, d_geom)\n","      num_evals = num_evals + num_evals_ADAPTIVE\n","\n","    if prob_dist.nature_of_noise == 'subsampling':\n","      event_time, delta_U_tilde, G_tilde, c_t_squared, log_grad_list, num_evals_ADAPTIVE =\\\n","      analytic_Bi_adaptive_thinning(current_location, v, array_for_lin_reg, lin_reg_delta_U_list, \n","                                    hat_beta_0, hat_beta_1, Sigma_cov_mat, prob_dist, \n","                                    num_lin_reg, k, d_geom, nature_of_noise='subsampling', \n","                                    sequence_of_log_grad_lists = sequence_of_log_grad_lists)\n","      num_evals = num_evals + num_evals_ADAPTIVE\n","\n","    \n","    # Step 3: if a bounce occur, hurray! If not, there are things to do...\n","    if (not event_time == 'no_events'):\n","      time_coasted = time_coasted + event_time\n","\n","      if prob_dist.nature_of_noise == 'artificial_gaus':\n","        next_array_of_singleton_ob = np.array([[0, -G_tilde, c_t_squared]])\n","        next_singleton_ob_delta_U = delta_U_tilde\n","        next_singleton_ob_log_grad_list = 'N/A'\n","\n","      \n","      if prob_dist.nature_of_noise == 'subsampling':\n","        reflected_v = reflection(v, delta_U_tilde)\n","        Equation_10_last_bit = log_grad_list.dot(reflected_v)\n","        new_c_t_squared = (prob_dist.num_obs**2)*(1 - prob_dist.n/prob_dist.num_obs)*np.var(Equation_10_last_bit)\n","\n","        next_array_of_singleton_ob = np.array([[0, -G_tilde, new_c_t_squared]])\n","        next_singleton_ob_delta_U = delta_U_tilde\n","        next_singleton_ob_log_grad_list = log_grad_list\n","\n","      termination = 1\n","    \n","    elif event_time == 'no_events':\n","      # update prior beliefs about mu_0, mu_1 (idea behind is because of the continuously differentiable\n","      # geometry of the energy function).\n","#      mu_0 = hat_beta_0\n","#      mu_1 = hat_beta_1\n","\n","      current_location = current_location + d_geom*normalised_v\n","      INITIAL_delta_U_tilde, INITIAL_G_tilde, INITIAL_c_t_squared, INITIAL_log_grad_list =\\\n","      eval_G_tilde_plus_emp_var(current_location, v, prob_dist)\n","      initialised_array_of_singleton_ob = np.array([[0, INITIAL_G_tilde, INITIAL_c_t_squared]])\n","\n","      initialised_singleton_delta_U = INITIAL_delta_U_tilde\n","      initialised_singleton_log_grad_list = INITIAL_log_grad_list\n","\n","\n","      num_evals = num_evals + 1\n","\n","      time_coasted = time_coasted + t_geom\n","\n","    regression_index = regression_index + 1\n","\n","\n","\n","  return time_coasted, delta_U_tilde, next_array_of_singleton_ob, next_singleton_ob_delta_U, next_singleton_ob_log_grad_list, num_evals\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gma6Nles0yF","colab_type":"text"},"source":["# Function: analytic_Bi_adaptive_thinning"]},{"cell_type":"markdown","metadata":{"id":"u5IjAQiUtEbZ","colab_type":"text"},"source":["analytic_Bi stands for \"analytic bisection\"."]},{"cell_type":"code","metadata":{"id":"zLocUuzptD2C","colab_type":"code","colab":{}},"source":["# Let Lambda be the upper-bound intensity function.\n","# Let Lambda = max(0, Rho).\n","\n","def analytic_Bi_adaptive_thinning(x, v, array_for_lin_reg, lin_reg_delta_U_list, hat_beta_0, hat_beta_1, \n","                                  Sigma_cov_mat, prob_dist, num_lin_reg, k, d_geom, \n","                                  nature_of_noise='artificial_gaus', \n","                                  sequence_of_log_grad_lists = 'None', num_bisections = 17):\n","  \n","  num_evals_ADAPTIVE = 0\n","\n","  # Step 0: define the Rho function\n","  Rho = lambda t: hat_beta_0 + hat_beta_1*t + k*np.sqrt(Sigma_cov_mat[0,0] + 2*Sigma_cov_mat[0,1]*t \n","                                                        + Sigma_cov_mat[1,1]*(t**2))\n","\n","\n","\n","  # Step 1: find all global time t such that Rho(t)=0. There is an exception\n","  #         when A=0, B=0, C=0.\n","  A, B, C, candidates = quadratic_equation_candidates_only(hat_beta_0, hat_beta_1, Sigma_cov_mat, k)\n","  \n","  \n","\n","\n","\n","\n","\n","\n","  # Step 2: Lambda = max(0, Rho). We need to determine the sign of Rho in different regions of [0,t_geom],\n","  #         by picking a point (the variable test_point below) and evaluate Rho on it.\n","  t_geom = d_geom/np.linalg.norm(v)\n","\n","  if candidates == 'no_candidates':\n","    boundaries_and_zeroes = [0, t_geom]\n","  else:\n","    # a new condition of (hat_beta_0 + hat_beta_1*x < 0) is added to reject roots.\n","    filter_candidates = [x for x in candidates if (0 < x) and (x < t_geom) \n","                         and (hat_beta_0 + hat_beta_1*x < 0)]\n","    boundaries_and_zeroes = [0] + filter_candidates + [t_geom]\n","  \n","  B_and_Z_length = len(boundaries_and_zeroes)\n","  \n","  # 1 means that the region has a positive sign of Rho. 0 means otherwise.\n","  signs_of_regions = []\n","\n","  for i in range(B_and_Z_length - 1):\n","    test_point = 0.5*(boundaries_and_zeroes[i] + boundaries_and_zeroes[i+1])\n","\n","    if Rho(test_point) > 0:\n","      signs_of_regions.append(1)\n","    else:\n","      signs_of_regions.append(0)\n","\n","\n","\n","  # Step 3: Note that Rho can be integrated analytically. Hence, We need to construct an \n","  # anti-derivative for it.\n","  a = Sigma_cov_mat[1,1]\n","  b = Sigma_cov_mat[0,0] - (Sigma_cov_mat[0,1]**2)/Sigma_cov_mat[1,1]\n","  c = Sigma_cov_mat[0,1]/Sigma_cov_mat[1,1]\n","\n","  complicated_1 = lambda t: (t + c)*np.sqrt(a*((t + c)**2) + b)\n","  complicated_2 = lambda t: (b/np.sqrt(a))*np.log(np.sqrt(a)*np.sqrt(a*((t + c)**2) + b) + a*(t + c))   if (not b == 0)   else  0\n","\n","  Anti_derivative = lambda t: hat_beta_0*t + 0.5*hat_beta_1*(t**2) +\\\n","  0.5*k*(complicated_1(t) + complicated_2(t))\n","\n","\n","\n","  # Step 4: compute intensity in each region.\n","  intensities_of_individual_regions = []\n","  cumulative_intensities_of_each_region = [0]\n","  \n","  for i in range(B_and_Z_length - 1):\n","    if signs_of_regions[i] == 0:\n","      intensities_of_individual_regions.append(0)\n","      \n","      cumulative_intensities_of_each_region.append(cumulative_intensities_of_each_region[-1])\n","\n","    if signs_of_regions[i] == 1:\n","      regional_intensity = (Anti_derivative(boundaries_and_zeroes[i+1]) - \n","                            Anti_derivative(boundaries_and_zeroes[i]))\n","      \n","      intensities_of_individual_regions.append(regional_intensity)\n","      cumulative_intensities_of_each_region.append(cumulative_intensities_of_each_region[-1] + \n","                                                   regional_intensity)\n","\n","  cumulative_intensities_of_each_region = cumulative_intensities_of_each_region[1:]\n","\n","\n","\n","\n","  # Step 5: Simulate the first arrival of the Cox Process via adaptive thinning on [0, t_geom].\n","  #         Note that it is possible that our upper-bound NHPP (or our Cox Process) has no \n","  #         arrivals at all.\n","\n","\n","  # We are going to simulate events of our upper-bound NHPP in [0, t_geom] until having an acceptance,\n","  # or until that the whole period [0, t_geom] has been simulated. We will accumulate the 'rejected' \n","  # inverse-transform intensities in this variable.\n","  accumulated_intensity = 0\n","\n","  termination = 0\n","\n","  while termination == 0:\n","    V = np.random.uniform()*(-1) + 1\n","    accumulated_intensity = accumulated_intensity - np.log(V)\n","\n","\n","\n","    # this is an indicator for absence of events in NHPP\n","    i_no_occurrence = 1\n","    ith_region = 0\n","\n","    while (i_no_occurrence == 1) and (ith_region <= (B_and_Z_length - 2)):\n","      if accumulated_intensity == cumulative_intensities_of_each_region[ith_region]:\n","        proposal_event_time = boundaries_and_zeroes[ith_region+1]\n","        i_no_occurrence = 0\n","    \n","      elif accumulated_intensity < cumulative_intensities_of_each_region[ith_region]:\n","        if ith_region == 0:\n","          remaining_intensity = accumulated_intensity\n","        \n","        elif ith_region > 0:\n","          remaining_intensity = accumulated_intensity - cumulative_intensities_of_each_region[ith_region-1]\n","      \n","        # We consider the following function to have the following interval domain:\n","        # [current_region_lower_bound, current_region_upper_bound]\n","        twisted_integral = lambda t: Anti_derivative(t) - Anti_derivative(boundaries_and_zeroes[ith_region])\\\n","        - remaining_intensity\n","      \n","        proposal_event_time = Bisection_Method_strict_increase(twisted_integral, boundaries_and_zeroes[ith_region], \n","                                                             boundaries_and_zeroes[ith_region+1], num_bisections)\n","        i_no_occurrence = 0\n","      \n","      ith_region = ith_region + 1\n","    \n","    if i_no_occurrence == 1:\n","      event_time = 'no_events'\n","      delta_U_tilde = 'N/A'\n","      G_tilde = 'N/A'\n","      c_t_squared = 'N/A'\n","      log_grad_list = 'N/A'\n","      termination = 1\n","  \n","    elif i_no_occurrence == 0:\n","      # acceptance/rejection step\n","      proposal_intensity = max(0, Rho(proposal_event_time))\n","      proposal_location = x + v*proposal_event_time\n","\n","\n","      # A situation that is unlikely to occur in the real computational environment: proposal_event_time\n","      # is one of those times used for performing linear regression [0, t_geom/(k-1), ..., t_geom].\n","      # If this really occurs, then we should not re-evaluate noisy gradient and G_tilde again.\n","      # Inside each nature_of_noise case, I tackle this unlikely situation, because I feel responsible\n","      # for coding in more things for the research of the PINTS group.\n","      \n","      if nature_of_noise == 'artificial_gaus':\n","\n","        # i_lin_reg_time is an indicator for whether the proposal time is a time instant\n","        # used in performing linear regression.\n","        i_lin_reg_time = 0\n","        lin_reg_time_list = array_for_lin_reg[:,0]\n","        \n","        # debugging\n","#        if (not np.array_equal(lin_reg_time_list, np.arange(num_lin_reg)*t_geom/(num_lin_reg-1))):\n","#          print('error in analytic_Bi: timing of linear regression')\n","#          print('lin_reg_time_list is: {}'.format(lin_reg_time_list))\n","#          print('The supposed-to-be-correct is: {}'.format(np.arange(num_lin_reg)*t_geom/(num_lin_reg-1)))\n","#        if (not len(lin_reg_time_list) == num_lin_reg):\n","#          print('error in analytic_Bi: length of timing of linear regression')\n","      \n","        for i in range(num_lin_reg):\n","          if proposal_event_time == lin_reg_time_list[i]:\n","            i_lin_reg_time = ['revisited ith', i]\n","\n","        if (not i_lin_reg_time == 0):\n","          lin_reg_time, G_tilde, c_t_squared = array_for_lin_reg[i_lin_reg_time[1]]\n","          delta_U_tilde = lin_reg_delta_U_list[i_lin_reg_time[1]]\n","\n","          log_grad_list = 'N/A'\n","        \n","\n","        elif i_lin_reg_time == 0:\n","          delta_U_tilde, G_tilde, c_t_squared, log_grad_list =\\\n","          eval_G_tilde_plus_emp_var(proposal_location, v, prob_dist)\n","\n","          num_evals_ADAPTIVE = num_evals_ADAPTIVE + 1\n","\n","        real_intensity = max(0, G_tilde)\n","        \n","        U = np.random.uniform()\n","        if U <= probability_of_acceptance(real_intensity, proposal_intensity):\n","          event_time = proposal_event_time\n","          termination = 1\n","\n","      \n","\n","\n","      if nature_of_noise == 'subsampling':\n","\n","        i_lin_reg_time = 0\n","        lin_reg_time_list = array_for_lin_reg[:,0]\n","        \n","        # debugging\n","#        if (not lin_reg_time_list == np.arange(num_lin_reg)*t_geom/(num_lin_reg-1)):\n","#          print('error in analytic_Bi: timing of linear regression')\n","#          print('lin_reg_time_list is: {}'.format(lin_reg_time_list))\n","#        if (not len(lin_reg_time_list) == num_lin_reg):\n","#          print('error in analytic_Bi: length of timing of linear regression')\n","      \n","        for i in range(num_lin_reg):\n","          if proposal_event_time == lin_reg_time_list[i]:\n","            i_lin_reg_time = ['revisited ith', i]\n","\n","        if (not i_lin_reg_time == 0):\n","          lin_reg_time, G_tilde, c_t_squared = array_for_lin_reg[i_lin_reg_time[1]]\n","          delta_U_tilde = lin_reg_delta_U_list[i_lin_reg_time[1]]\n","\n","          log_grad_list = sequence_of_log_grad_lists[i_lin_reg_time[1]]\n","        \n","\n","        elif i_lin_reg_time == 0:\n","          delta_U_tilde, G_tilde, c_t_squared, log_grad_list =\\\n","          eval_G_tilde_plus_emp_var(proposal_location, v, prob_dist)\n","\n","          num_evals_ADAPTIVE = num_evals_ADAPTIVE + 1\n","\n","        real_intensity = max(0, G_tilde)\n","        \n","        U = np.random.uniform()\n","        if U <= probability_of_acceptance(real_intensity, proposal_intensity):\n","          event_time = proposal_event_time\n","          termination = 1\n","  \n","\n","\n","  return event_time, delta_U_tilde, G_tilde, c_t_squared, log_grad_list, num_evals_ADAPTIVE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqiZMh_8hsUb","colab_type":"text"},"source":["# Function: Roots of Quadratic Equations"]},{"cell_type":"markdown","metadata":{"id":"32dpTgnlh2D7","colab_type":"text"},"source":["This function finds possible candidate of real roots of certain quadratic equations with some features, Ax^2+Bx+C=0.\n","\n","It also contain the step of rejecting candidate time t such that Rho(t) is nonzero.\n","\n","For more info, please refer to the MSc report (pp. ??-??)."]},{"cell_type":"code","metadata":{"id":"bEoWpwLhhGje","colab_type":"code","colab":{}},"source":["def quadratic_equation_candidates_only(hat_beta_0, hat_beta_1, Sigma_cov_mat, k):\n","\n","  A = (k**2)*Sigma_cov_mat[1,1] - hat_beta_1**2\n","  B = 2*((k**2)*Sigma_cov_mat[0,1] - hat_beta_0*hat_beta_1)\n","  C = (k**2)*Sigma_cov_mat[0,0] - hat_beta_0**2\n","\n","  if abs(A) < 10**(-10):\n","    A = 0\n","  if abs(B) < 10**(-10):\n","    B = 0\n","  if abs(C) < 10**(-10):\n","    C = 0\n","\n","\n","  if A==0 and B==0 and C==0:\n","#    print('1')\n","    candidates = [-hat_beta_0/hat_beta_1]\n","\n","\n","  elif A==0 and B==0 and (not C==0):\n","#    print('2')\n","    candidates = 'no_candidates'\n","\n","\n","  elif A==0 and (not B==0) and (not C==0):\n","#    print('3')\n","    # check if (-C/B) really produces zero intensity\n","    if hat_beta_0 + hat_beta_1*(-C/B) < 0:\n","      candidates = [-C/B]\n","    else:\n","      candidates = 'no_candidates'\n","  \n","\n","  elif A==0 and (not B==0) and C==0:\n","#    print('4')\n","    candidates = 'no_candidates'\n","\n","\n","\n","  else:      # this means that A is non-zero:\n","#    print('5')\n","    discriminant = B**2 - 4*A*C\n","    \n","    if discriminant < 0:\n","#      print('5.1')\n","      candidates = 'no_candidates'\n","    \n","    elif discriminant == 0:\n","#      print('5.2')\n","      sol = -B/(2*A)\n","      if hat_beta_0 + hat_beta_1*(sol) < 0:\n","        candidates = [sol]\n","      else:\n","        candidates = 'no_candidates'\n","\n","\n","\n","    \n","    elif discriminant > 0:\n","#      print('5.3')\n","      sol_1 = -B/(2*A) - np.sqrt(discriminant)/(2*A)\n","      sol_2 = -B/(2*A) + np.sqrt(discriminant)/(2*A)\n","\n","      if hat_beta_0 + hat_beta_1*(sol_1) < 0 and hat_beta_0 + hat_beta_1*(sol_2) < 0:\n","        candidates = [sol_1, sol_2]\n","\n","      elif hat_beta_0 + hat_beta_1*(sol_1) < 0 and (not hat_beta_0 + hat_beta_1*(sol_2) < 0):\n","        candidates = [sol_1]\n","\n","      elif (not hat_beta_0 + hat_beta_1*(sol_1) < 0) and hat_beta_0 + hat_beta_1*(sol_2) < 0:\n","        candidates = [sol_2]\n","\n","      else:\n","        candidates = 'no_candidates'\n","\n","\n","  return A, B, C, candidates\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_3UQjzhCJE2","colab_type":"text"},"source":["# Function: Bisection Method"]},{"cell_type":"markdown","metadata":{"id":"uY0vguOUCXrX","colab_type":"text"},"source":["Because it seems that there are some unwanted complications involving xtol, rtol and maxiter of scipy.optimize.bisect (see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.bisect.html),\n","\n","I will build my own bisection method."]},{"cell_type":"markdown","metadata":{"id":"udZyu2uEEd5N","colab_type":"text"},"source":["We want to find a root of F within [a,b].\n","\n","We will only work with strictly increasing F.\n","\n","We also assume that (F(a) <= 0) and (F(b) >= 0)."]},{"cell_type":"code","metadata":{"id":"KDIwT202COg4","colab_type":"code","colab":{}},"source":["# num_iters is the number of bisections to be performed.\n","\n","\n","def Bisection_Method_strict_increase(F, a, b, num_iters):\n","  exact_sol = 'bla'\n","  i_exact_sol = 0\n","  \n","  if F(a) == 0:\n","    exact_sol = a\n","    i_exact_sol = 1\n","  \n","  elif F(b) == 0:\n","    exact_sol = b\n","    i_exact_sol = 1\n","  \n","  else:\n","    lower_bound = a\n","    upper_bound = b\n","\n","    for i in range(num_iters):\n","      mid_point = 0.5*(lower_bound + upper_bound)\n","\n","      if F(mid_point) == 0:\n","        exact_sol = mid_point\n","        i_exact_sol = 1\n","        break\n","\n","      elif F(mid_point) < 0:\n","        lower_bound = mid_point\n","      \n","      elif F(mid_point) > 0:\n","        upper_bound = mid_point\n","    \n","    approx_sol = 0.5*(lower_bound + upper_bound)\n","\n","  \n","  if i_exact_sol == 1:\n","    return exact_sol\n","\n","  elif i_exact_sol == 0:\n","    return approx_sol\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FoVjooJreVoJ","colab_type":"text"},"source":["# Noisy Gaussian distribution"]},{"cell_type":"code","metadata":{"id":"ONz9mEhNFSKO","colab_type":"code","colab":{}},"source":["class Noisy_Gaussian:\n","    \n","    def __init__(self, mean, cov_mat, noise_presence, exact_gaus_noise_var, \n","                 nature_of_noise = 'artificial_gaus'):\n","        self.mean = mean\n","        self.cov_mat = cov_mat\n","        self.dim = mean.size\n","\n","        self.noise_presence = noise_presence\n","        self.exact_gaus_noise_var = exact_gaus_noise_var\n","        self.nature_of_noise = 'artificial_gaus'\n","    \n","    \n","    def energy(self, x): # constant coeff is discarded\n","        return 0.5*((x - self.mean).dot(np.linalg.inv(self.cov_mat)).dot(x - self.mean)) \n","        \n","   \n","    def ener_grad(self, x):\n","#      if noise_presence == True:\n","#        noise = np.sqrt(self.exact_gaus_noise_var)*np.random.standard_normal(self.dim)\n","#        return (np.linalg.inv(self.cov_mat)).dot(x - self.mean) + noise\n","#      \n","#      else:\n","      return (np.linalg.inv(self.cov_mat)).dot(x - self.mean)"],"execution_count":null,"outputs":[]}]}