{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Github_SBPS_new_implementation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qHXHYcO97umJ"},"source":["# Import Python packages"]},{"cell_type":"code","metadata":{"id":"wPA9b-0Gzxss"},"source":["%cd /content/drive/My\\ Drive/msc_dissertation/pints-master/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bp8aFY5-yS-U"},"source":["!python setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGMvlzc48os5"},"source":["!pip install cma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zgs5lhQMTid-"},"source":["import numpy as np\n","import math\n","import numpy.matlib as matlib\n","import matplotlib.pyplot as plt\n","import operator\n","\n","from time import perf_counter\n","import scipy.stats\n","import scipy\n","\n","import os\n","import os.path\n","\n","\n","import matplotlib\n","import cma\n","import tabulate\n","import pints\n","import pints.toy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBiq_8Z_rJq0"},"source":["# SBPS MCMC algorithms"]},{"cell_type":"markdown","metadata":{"id":"cWPTgWidswNU"},"source":["## Function: SBPS MCMC"]},{"cell_type":"code","metadata":{"id":"YZLj8U0erM68"},"source":["# This is the function we call the SBPS MCMC.\n","# It will output the turning points of the trajectory, velocities, times there,\n","# cpu seconds used there, and number of pdf evaluations used there.\n","\n","\n","\n","# num_lin_reg is the hyper-param for num of points used to do linear regression.\n","def new_SBPS(x0, v0, Time, lambda_ref, prob_dist, num_lin_reg, k, d_geom):\n","    \n","    cpu_time_executed = perf_counter()\n","    computational_times = [0]\n","    start_time = cpu_time_executed\n","    \n","    turn_pts = [x0]\n","    list_of_velo = [v0]\n","    striding_times = [0]\n","    total_evals_list = [1]\n","\n","    recorded_variances_arr = np.array([[-1]*num_lin_reg])\n","\n","    \n","    # the following records whether bias is introduced because the upper-bound intensity goes\n","    # below the realised intensity function of the Cox Process\n","    bias_list = [0]\n","    cumulative_bias_list = [0]\n","    total_bias = 0\n","    \n","    dim = x0.size\n","    i = 1\n","    x = x0\n","    v = v0\n","    t = 0\n","    total_evals = 1\n","    \n","    next_singleton_ob_delta_U, G_tilde, c_t_squared, next_singleton_ob_log_grad_list =\\\n","    eval_G_tilde_plus_emp_var(x, v, prob_dist)\n","    next_array_of_singleton_ob = np.array([[0, G_tilde, c_t_squared]])    # time; G_tilde; variance\n","    \n","    \n","    while t < Time:\n","        # Next bounce\n","        # next_array_of_obs is a np.array being [0, G_t, variance]\n","\n","        if i % 500 == 1:\n","          tau_bounce, reflected_v, next_array_of_singleton_ob, next_singleton_ob_delta_U,\\\n","          next_singleton_ob_log_grad_list, num_evals, i_bias, recorded_variances =\\\n","          New_simulation_Cox_Process(x, v, next_array_of_singleton_ob, next_singleton_ob_delta_U, \n","                                     next_singleton_ob_log_grad_list, prob_dist, num_lin_reg, k, d_geom,\n","                                     recorded_variances = True)\n","          \n","          recorded_variances_arr = np.concatenate((recorded_variances_arr, np.array([recorded_variances])), \n","                                                  axis = 0)\n","\n","        else:\n","          tau_bounce, reflected_v, next_array_of_singleton_ob, next_singleton_ob_delta_U,\\\n","          next_singleton_ob_log_grad_list, num_evals, i_bias =\\\n","          New_simulation_Cox_Process(x, v, next_array_of_singleton_ob, next_singleton_ob_delta_U, \n","                                     next_singleton_ob_log_grad_list, prob_dist, num_lin_reg, k, d_geom,\n","                                     recorded_variances = False)\n","\n","        total_evals = total_evals + num_evals\n","        \n","        # Next refreshment\n","        beta = 1/lambda_ref\n","        tau_ref = np.random.exponential(scale = beta)\n","    \n","    \n","        tau = min(tau_bounce, tau_ref)\n","        x = x + tau*v\n","        t = t + tau\n","        \n","        # Update the velocity after a bounce/refreshment\n","        if tau_ref < tau_bounce:\n","            v = np.random.standard_normal(dim)\n","            next_singleton_ob_delta_U, next_G_tilde, next_c_t_squared, next_singleton_ob_log_grad_list =\\\n","            eval_G_tilde_plus_emp_var(x, v, prob_dist)\n","            next_array_of_singleton_ob = np.array([[0, next_G_tilde, next_c_t_squared]])\n","            \n","            total_evals = total_evals + 1\n","        else:\n","            v = reflected_v\n","            \n","        \n","        cpu_time_executed = perf_counter() - start_time\n","        \n","        list_of_velo.append(v)\n","        turn_pts.append(x)\n","        striding_times.append(t)\n","        computational_times.append(cpu_time_executed)\n","        total_evals_list.append(total_evals)\n","\n","        total_bias = total_bias + i_bias\n","        bias_list.append(i_bias)\n","        cumulative_bias_list.append(total_bias)\n","\n","        if i%50 == 0:\n","          print('{}th turns   current time: {}   cpu: {}   evals: {}'.format(i, t, cpu_time_executed,\n","                                                                             total_evals))\n","          print('Location after event: {}'.format(x))\n","          print('total bias: {}'.format(total_bias))\n","        \n","\n","        if i%200 == 0:\n","          path_to_save_chain = 'SBPS_7_BLR_3/SBPS_7_BLR_3_i_' + str(i) + '_'\n","          np.savez(path_to_save_chain + 'turning_points', turning_points = np.array(turn_pts))\n","          np.savez(path_to_save_chain + 'v_list', v_list = np.array(list_of_velo))\n","          np.savez(path_to_save_chain + 'stride_times', stride_times = np.array(striding_times))\n","          np.savez(path_to_save_chain + 'evaluations_list', evaluations_list = np.array(total_evals_list))\n","          np.savez(path_to_save_chain + 'CPUtime_list', CPUtime_list = np.array(computational_times))\n","          np.savez(path_to_save_chain + 'bias_list', bias_list = np.array(bias_list))\n","          np.savez(path_to_save_chain + 'cumulative_bias_list', cumulative_bias_list = np.array(cumulative_bias_list))\n","          np.savez(path_to_save_chain + 'recorded_var_array', recorded_var_array = recorded_variances_arr)\n","          print('saved at {}th change'.format(i))\n","        \n","        i = i+1\n","\n","    recorded_variances_arr = np.delete(recorded_variances_arr, 0, 0)\n","\n","    \n","\n","\n","\n","    return turn_pts, list_of_velo, striding_times, total_evals_list, computational_times, \\\n","    bias_list, cumulative_bias_list, recorded_variances_arr\n","    # x_list, v_list, t_list, evals_list, cpu_time_list, bias_list, cumulative_bias_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nO1bDmqOaAiV"},"source":["## Function: reflection of velocity"]},{"cell_type":"code","metadata":{"id":"Oisi36eEZOEd"},"source":["def reflection(v, observed_grad):\n","    \n","    return v - 2*(np.sum(observed_grad*v)/(np.sum(observed_grad*observed_grad)))*observed_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FI_IN8dkaaR4"},"source":["## Function: x_v_t_arbitrary_times"]},{"cell_type":"markdown","metadata":{"id":"04VSaVMqkRz3"},"source":["### Version 1"]},{"cell_type":"code","metadata":{"id":"APl-5WoQa5Wq"},"source":["\n","def x_v_t_arbitrary_times(turn_pts, list_of_velo, striding_times, intermediate_times):\n","    \n","    num_changes = len(turn_pts)\n","    num_required_times = len(intermediate_times)\n","    tiling_interm_times = np.transpose(np.tile(intermediate_times, (num_changes, 1)))\n","    testing_mat_1 = tiling_interm_times - np.tile(striding_times, (num_required_times, 1))\n","    testing_mat_2 = np.where(testing_mat_1 >= 0, 1, 0)\n","    indices_no_later_than = np.sum(testing_mat_2, axis = 1) - 1\n","        \n","    turn_pts_no_later = [turn_pts[i] for i in indices_no_later_than]\n","    velo_no_later = [list_of_velo[i] for i in indices_no_later_than]\n","    stride_time_no_later = [striding_times[i] for i in indices_no_later_than]\n","    \n","       \n","    interm_times = list(intermediate_times)\n","        \n","    list_of_coasting_times = list(map(operator.sub, interm_times, stride_time_no_later))\n","    distance_strided = list(map(operator.mul, list_of_coasting_times, velo_no_later))\n","    locations_at_required_times = list(map(operator.add, turn_pts_no_later, distance_strided))\n","        \n","\n","    return locations_at_required_times, velo_no_later, interm_times\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jGXg9gFIuyu"},"source":["### Version 2"]},{"cell_type":"code","metadata":{"id":"AifqqqYgGNnQ"},"source":["def x_v_t_arbitrary_times_NParrays(turn_pts, list_of_velo, striding_times, intermediate_times):\n","    \n","    # turn_pts, list_of_velo, striding_times, intermediate_times can be list or np.array\n","\n","    num_changes = len(turn_pts)\n","    num_required_times = len(intermediate_times)\n","    tiling_interm_times = np.transpose(np.tile(intermediate_times, (num_changes, 1)))\n","    testing_mat_1 = tiling_interm_times - np.tile(striding_times, (num_required_times, 1))\n","    testing_mat_2 = np.where(testing_mat_1 >= 0, 1, 0)\n","    indices_no_later_than = np.sum(testing_mat_2, axis = 1) - 1\n","        \n","    turn_pts_no_later = np.array([turn_pts[i] for i in indices_no_later_than])\n","    velo_no_later = np.array([list_of_velo[i] for i in indices_no_later_than])\n","    stride_time_no_later = np.array([striding_times[i] for i in indices_no_later_than])\n","    \n","        \n","    list_of_coasting_times = intermediate_times - stride_time_no_later\n","    distance_strided = np.transpose(np.multiply(np.transpose(velo_no_later), list_of_coasting_times))\n","    locations_at_required_times = turn_pts_no_later + distance_strided\n","        \n","\n","    return locations_at_required_times, velo_no_later, intermediate_times"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGs1ehZsfS0M"},"source":["## Function: Probability of Acceptance"]},{"cell_type":"code","metadata":{"id":"_YQv65k-fWuV"},"source":["\n","def probability_of_acceptance(real_intensity, proposal_intensity):  # [G(t)]_{+}; lambda(t)\n","    \n","    if proposal_intensity > 0:\n","        return min(1, real_intensity/proposal_intensity)\n","    \n","    elif proposal_intensity == 0 and real_intensity > 0:\n","        return 1\n","    \n","    elif proposal_intensity == 0 and real_intensity == 0:\n","        return 0.5\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71iYpJ4V9TRx"},"source":["## Function: eval_G_tilde_plus_emp_var"]},{"cell_type":"code","metadata":{"id":"rckEVDgW9Vlc"},"source":["def eval_G_tilde_plus_emp_var(x, v, prob_dist):\n","    \n","    if prob_dist.nature_of_noise == 'artificial_gaus':\n","        \n","        delta_U_mean = prob_dist.ener_grad(x)\n","        gaus_noise_vec = np.random.multivariate_normal(np.zeros(x.size), np.identity(x.size)*prob_dist.exact_gaus_noise_var)\n","        \n","        delta_U_tilde = delta_U_mean + gaus_noise_vec\n","        # print('delta_U_tilde:', delta_U_tilde)\n","        # print('v: ', v)\n","        \n","        G_tilde = v.dot(delta_U_tilde)\n","        \n","        c_t_squared = (v.dot(v))*prob_dist.exact_gaus_noise_var\n","        \n","        log_grad_list = 'N/A'\n","        \n","        \n","    elif prob_dist.nature_of_noise == 'subsampling':\n","        \n","        # log_grad is not the energy gradient; Equation (10) of the paper is followed.\n","        delta_U_tilde, log_grad_list = prob_dist.ener_grad(x, list_sampling = True)\n","        dot_prod_list = log_grad_list.dot(v)\n","        \n","        G_tilde = v.dot(delta_U_tilde)\n","        \n","        c_t_squared = (prob_dist.num_obs**2)*(1 - prob_dist.n/prob_dist.num_obs)*np.var(dot_prod_list)\n","        \n","        # if c_t_squared = 0, set it to a small positive value.\n","        if c_t_squared == 0:\n","          c_t_squared = 0.000001\n","        \n","\n","    return delta_U_tilde, G_tilde, c_t_squared, log_grad_list\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr2uCcgLK9zI"},"source":["## Function: Bayesian Linear Regression (assuming Gaussian Priors on beta0, beta1)"]},{"cell_type":"code","metadata":{"id":"vkXi93u1K_17"},"source":["\n","def Bayesian_Linear_Regression_doub_gaus(array_of_observations, sig_0, mu_0, sig_1, mu_1):\n","    \n","    # print(array_of_observations)\n","    \n","    array_of_times = array_of_observations[:,0]\n","    array_of_Gs = array_of_observations[:,1]\n","    array_of_variances = array_of_observations[:,2]\n","    \n","    S_1 = np.sum(1/array_of_variances)/2\n","    S_2 = np.sum((1/array_of_variances)*(array_of_times))/2\n","    S_3 = np.sum((1/array_of_variances)*(array_of_Gs))/2\n","    \n","    S_4 = np.sum((1/array_of_variances)*(array_of_times)*(array_of_Gs))/2\n","    S_5 = np.sum((1/array_of_variances)*(array_of_times)*(array_of_times))/2\n","    S_6 = np.sum((1/array_of_variances)*(array_of_Gs)*(array_of_Gs))/2\n","    \n","    \n","    D_00 = -2*S_1 - 1/(sig_0**2)\n","    D_01 = -2*S_2\n","    D_02 = mu_0/(sig_0**2) + 2*S_3\n","    \n","    D_10 = -2*S_2\n","    D_11 = -2*S_5 - 1/(sig_1**2)\n","    D_12 = mu_1/(sig_1**2) + 2*S_4\n","    \n","    \n","    if D_01 == 0:\n","        \n","        expect_beta0 = -D_02/D_00\n","        expect_beta1 = -D_12/D_11\n","        expect_beta0_squared = (D_02/D_00)**2 - 1/D_00\n","        expect_beta1_squared = (D_12/D_11)**2 - 1/D_11\n","        expect_beta0_beta1 = (D_02*D_12)/(D_00*D_11)\n","    \n","    \n","    \n","    else:\n","        expect_beta0 = (D_01*D_12 - D_02*D_11)/(D_00*D_11 - D_01*D_10)\n","        expect_beta1 = (D_10*D_02 - D_12*D_00)/(D_00*D_11 - D_01*D_10)\n","        expect_beta0_squared = ((D_01*D_12 - D_02*D_11)/(D_00*D_11 - D_01*D_10))**2 + D_11/(D_01*D_10 - D_00*D_11)\n","        expect_beta1_squared = ((D_10*D_02 - D_12*D_00)/(D_00*D_11 - D_01*D_10))**2 + D_00/(D_01*D_10 - D_00*D_11)\n","        expect_beta0_beta1 = expect_beta0*expect_beta1 + D_10/(D_00*D_11 - D_01*D_10)\n","    \n","    \n","    hat_beta_0 = expect_beta0\n","    hat_beta_1 = expect_beta1\n","    \n","    Var_0 = expect_beta0_squared - (expect_beta0)**2\n","    Var_1 = expect_beta1_squared - (expect_beta1)**2\n","    Cov_01 = expect_beta0_beta1 - (expect_beta0)*(expect_beta1)\n","    \n","    Sigma_cov_mat = np.array([[Var_0, Cov_01], [Cov_01, Var_1]])\n","    \n","\n","    \n","    # need to calculate E[beta_0], E[beta_1], E[beta_1^2], E[beta_1*beta_2], E[beta_2^2]\n","    \n","    return hat_beta_0, hat_beta_1, Sigma_cov_mat\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g660bqWf3984"},"source":["## Function: Bayesian Linear Regression (Bayesian Lasso)\n"]},{"cell_type":"code","metadata":{"id":"s4G0lmXg4FF8"},"source":["def Bayesian_Linear_Regression_Lasso(array_of_observations, sig_1, mu_1):\n","  \n","  # standardising the time points\n","  array_of_times = array_of_observations[:,0]\n","#  print('array_of_times: {}'. format(array_of_times))\n","  mean_of_times = np.mean(array_of_times)\n","#  print('mean_of_times: {}'. format(mean_of_times))\n","  SD_of_times = np.sqrt(float(np.cov(array_of_times)))\n","#  print('SD_of_times: {}'. format(SD_of_times))\n","\n","\n","  # mean of G_tilde's\n","  array_of_ys = array_of_observations[:,1]\n","#  print('array_of_ys: {}'. format(array_of_ys))\n","  mean_of_ys = np.mean(array_of_ys)\n","#  print('mean_of_ys: {}'. format(mean_of_ys))\n","\n","\n","  # array of empirical variance\n","  array_of_vars = array_of_observations[:,2]\n","#  print('array_of_vars: {}'. format(array_of_vars))\n","\n","\n","  k_vec = (array_of_times - mean_of_times)/SD_of_times\n","#  print('k_vec: {}'. format(k_vec))\n","  l_vec = mean_of_ys - array_of_ys\n","#  print('l_vec: {}'. format(l_vec))\n","\n","  P_tilde = 0.5/(sig_1**2) + 0.5*np.sum(k_vec*k_vec/array_of_vars)\n","#  print('P_tilde: {}'. format(P_tilde))\n","  Q_tilde = np.sum(k_vec*l_vec/array_of_vars) - mu_1/(sig_1**2)\n","#  print('Q_tilde: {}'. format(Q_tilde))\n","\n","\n","  hat_beta_1 = -0.5*Q_tilde/(SD_of_times*P_tilde)\n","#  print('hat_beta_1: {}'. format(hat_beta_1))\n","  hat_beta_0 = mean_of_ys - hat_beta_1*mean_of_times\n","#  print('hat_beta_1: {}'. format(hat_beta_1))\n","  post_cov_11 = 0.5/((SD_of_times**2)*P_tilde)\n","  post_cov_01 = -post_cov_11*mean_of_times\n","  post_cov_00 = post_cov_11*(mean_of_times**2)\n","\n","  Sigma_cov_mat = np.array([[post_cov_00, post_cov_01],[post_cov_01, post_cov_11]])\n","#  print('Sigma_cov_mat: {}'. format(Sigma_cov_mat))\n","\n","\n","  return hat_beta_0, hat_beta_1, Sigma_cov_mat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kaNxLv60sn9J"},"source":["## Function: new_simulation_Cox_Process"]},{"cell_type":"markdown","metadata":{"id":"115H-i6xEfzN"},"source":["Propose one change: if it is calculated that $\\hat{\\beta}_{0}$, $\\hat{\\beta}_{1}$, $\\Sigma$ becomes `nan`, I will use the previous values of $\\hat{\\beta}_{0}$, $\\hat{\\beta}_{1}$, $\\Sigma$ to construct the upper-bound intensity.\n","\n","Here are the hypothetical initialsations right at the start:\n","\n","`previous_beta_0 = 1`\n","\n","`previous_beta_1 = 1`\n","\n","`previous_Sigma_cov_mat = np.identity(2)`"]},{"cell_type":"code","metadata":{"id":"unrMSKN7sy87"},"source":["\n","def New_simulation_Cox_Process(x, v, array_of_starting_singleton_ob, starting_singleton_delta_U, \n","                               starting_singleton_log_grad_list, prob_dist, num_lin_reg, k, d_geom,\n","                               recorded_variances):\n","\n","  dim = len(x)\n","  current_location = x\n","  \n","  termination = 0\n","  time_coasted = 0\n","  initialised_array_of_singleton_ob = array_of_starting_singleton_ob\n","  initialised_singleton_delta_U = starting_singleton_delta_U\n","  initialised_singleton_log_grad_list = starting_singleton_log_grad_list\n","\n","  num_evals = 0\n","  t_geom = d_geom/np.linalg.norm(v)\n","\n","  normalised_v = v/np.linalg.norm(v)\n","  tiling_v = np.tile(normalised_v, (num_lin_reg - 1, 1))\n","  mult_factors = np.transpose(np.tile(np.arange(1,num_lin_reg)*d_geom/(num_lin_reg-1), (dim,1)))\n","  moving_points_of_reg_obs = tiling_v*mult_factors\n","\n","  moving_times_for_reg = np.arange(1,num_lin_reg)*d_geom/((num_lin_reg-1)*np.linalg.norm(v))\n","\n","  # This counts the number of while loops performed in the following.\n","  regression_index = 0\n","\n","  sig_0 = 1\n","  sig_1 = 1\n","  mu_0 = 0\n","  mu_1 = 0\n","\n","  # Hypothetical initialisations right at the start.\n","  previous_beta_0 = 1\n","  previous_beta_1 = 1\n","  previous_Sigma_cov_mat = np.identity(2)\n","\n","\n","  while termination == 0:\n","    # test whether the SBPS MCMC is stuck here\n","    # investigate why setting d_geom to low value is problematic.\n","#    if regression_index % 10 == 9:\n","#      print('regression index: {}'.format(regression_index))\n","#      print('hat_beta_0: {}'.format(hat_beta_0))\n","#      print('hat_beta_1: {}'.format(hat_beta_1))\n","#      print('Sigma_cov_mat: {}'.format(Sigma_cov_mat))\n","    # Step 1: Bayesian Linear Regression\n","\n","    points_of_regression_obs = current_location + moving_points_of_reg_obs\n","    \n","    array_for_lin_reg = initialised_array_of_singleton_ob      # 2D array of dimension (1,3)\n","\n","    lin_reg_delta_U_list = [initialised_singleton_delta_U]\n","\n","    if prob_dist.nature_of_noise == 'artificial_gaus':\n","      \n","      for i in range(num_lin_reg-1):\n","        BAYES_delta_U_tilde, BAYES_G_tilde, BAYES_c_t_squared, BAYES_log_grad_list =\\\n","        eval_G_tilde_plus_emp_var(points_of_regression_obs[i], v, prob_dist)\n","        arr_observation = np.array([[moving_times_for_reg[i], BAYES_G_tilde, BAYES_c_t_squared]])\n","        \n","        array_for_lin_reg = np.concatenate((array_for_lin_reg, arr_observation), axis = 0)\n","\n","        lin_reg_delta_U_list.append(BAYES_delta_U_tilde)\n","\n","        # count the number of pdf evals\n","        num_evals = num_evals + 1\n","\n","    elif prob_dist.nature_of_noise == 'subsampling':\n","      \n","      sequence_of_log_grad_lists = [initialised_singleton_log_grad_list]\n","\n","      for i in range(num_lin_reg-1):\n","        BAYES_delta_U_tilde, BAYES_G_tilde, BAYES_c_t_squared, BAYES_log_grad_list =\\\n","        eval_G_tilde_plus_emp_var(points_of_regression_obs[i], v, prob_dist)\n","        arr_observation = np.array([[moving_times_for_reg[i], BAYES_G_tilde, BAYES_c_t_squared]])\n","        \n","        array_for_lin_reg = np.concatenate((array_for_lin_reg, arr_observation), axis = 0)\n","        \n","        lin_reg_delta_U_list.append(BAYES_delta_U_tilde)\n","\n","        sequence_of_log_grad_lists.append(BAYES_log_grad_list)\n","\n","        # count the number of pdf evals\n","        num_evals = num_evals + 1\n","    \n","    hat_beta_0, hat_beta_1, Sigma_cov_mat =\\\n","    Bayesian_Linear_Regression_Lasso(array_for_lin_reg, sig_1, mu_1)\n","\n","    if math.isnan(hat_beta_0) or math.isnan(hat_beta_1) or math.isnan(np.sum(Sigma_cov_mat)):\n","      hat_beta_0 = previous_beta_0\n","      hat_beta_1 = previous_beta_1\n","      Sigma_cov_mat = previous_Sigma_cov_mat\n","    \n","    else:\n","      previous_beta_0 = hat_beta_0\n","      previous_beta_1 = hat_beta_1\n","      previous_Sigma_cov_mat = Sigma_cov_mat\n","    \n","#    extrapolated_var = np.mean(array_for_lin_reg[:,2])\n","\n","#    if (not math.isinf(extrapolated_var)) and (not math.isnan(extrapolated_var)):\n","#      hat_beta_0 = hat_beta_0 + k*extrapolated_var\n","\n","#    else:\n","#      hat_beta_0 = hat_beta_0 + k*10000\n","\n","\n","    array_of_variances = array_for_lin_reg[:,2]\n","#    max_SD = np.sqrt(np.max(array_of_variances))\n","#    hat_beta_0 = hat_beta_0 + k*max_SD\n","    \n","\n","    # Step 2: Propose events from this upper NHPP. Perform acceptance-rejection\n","    # The first arrival of an NHPP with time duration [0, d_geom/|v|] is simulated.\n","    \n","    if prob_dist.nature_of_noise == 'artificial_gaus':\n","      event_time, delta_U_tilde, G_tilde, c_t_squared, log_grad_list, num_evals_ADAPTIVE, i_bias =\\\n","      analytic_Bi_adaptive_thinning(current_location, v, array_for_lin_reg, lin_reg_delta_U_list, \n","                                    hat_beta_0, hat_beta_1, Sigma_cov_mat, prob_dist, \n","                                    num_lin_reg, k, d_geom)\n","      num_evals = num_evals + num_evals_ADAPTIVE\n","\n","    elif prob_dist.nature_of_noise == 'subsampling':\n","      event_time, delta_U_tilde, G_tilde, c_t_squared, log_grad_list, num_evals_ADAPTIVE, i_bias =\\\n","      analytic_Bi_adaptive_thinning(current_location, v, array_for_lin_reg, lin_reg_delta_U_list, \n","                                    hat_beta_0, hat_beta_1, Sigma_cov_mat, prob_dist, \n","                                    num_lin_reg, k, d_geom, nature_of_noise='subsampling', \n","                                    sequence_of_log_grad_lists = sequence_of_log_grad_lists)\n","      num_evals = num_evals + num_evals_ADAPTIVE\n","\n","    \n","    # Step 3: if a bounce occur, hurray! If not, there are things to do...\n","    if (not event_time == 'no_events'):\n","      time_coasted = time_coasted + event_time\n","\n","      if prob_dist.nature_of_noise == 'artificial_gaus':\n","        reflected_v = reflection(v, delta_U_tilde)\n","        next_array_of_singleton_ob = np.array([[0, -G_tilde, c_t_squared]])\n","        next_singleton_ob_delta_U = delta_U_tilde\n","        next_singleton_ob_log_grad_list = 'N/A'\n","\n","      \n","      elif prob_dist.nature_of_noise == 'subsampling':\n","        reflected_v = reflection(v, delta_U_tilde)\n","        Equation_10_last_bit = log_grad_list.dot(reflected_v)\n","        \n","        new_c_t_squared = (prob_dist.num_obs**2)*(1 - prob_dist.n/prob_dist.num_obs)*\\\n","        np.var(Equation_10_last_bit)\n","\n","        next_array_of_singleton_ob = np.array([[0, -G_tilde, new_c_t_squared]])\n","        next_singleton_ob_delta_U = delta_U_tilde\n","        next_singleton_ob_log_grad_list = log_grad_list\n","\n","      termination = 1\n","    \n","    elif event_time == 'no_events':\n","      # update prior beliefs about mu_0, mu_1 (idea behind is because of the continuously differentiable\n","      # geometry of the energy function).\n","#      mu_0 = hat_beta_0\n","#      mu_1 = hat_beta_1\n","\n","      current_location = current_location + d_geom*normalised_v\n","      initialised_array_of_singleton_ob = np.array([[0, BAYES_G_tilde, BAYES_c_t_squared]])\n","\n","      initialised_singleton_delta_U = BAYES_delta_U_tilde\n","      initialised_singleton_log_grad_list = BAYES_log_grad_list\n","\n","\n","\n","      time_coasted = time_coasted + t_geom\n","    \n","    \n","    regression_index = regression_index + 1\n","\n","    if (regression_index > 0) and (regression_index % 10 == 0):\n","      print('regression_index: {}   location: {}'.format(regression_index, current_location))\n","#      time_coasted = math.inf\n","#      reflected_v = 'N/A'\n","#      next_array_of_singleton_ob = 'N/A'\n","#      next_singleton_ob_delta_U = 'N/A'\n","#      next_singleton_ob_log_grad_list = 'N/A'\n","#      array_of_variances = np.array([-1]*num_lin_reg)\n","\n","#      termination = 1\n","\n","    \n","\n","\n","  if recorded_variances == True:\n","    return time_coasted, reflected_v, next_array_of_singleton_ob, next_singleton_ob_delta_U, \\\n","    next_singleton_ob_log_grad_list, num_evals, i_bias, array_of_variances\n","\n","  else:\n","    return time_coasted, reflected_v, next_array_of_singleton_ob, next_singleton_ob_delta_U, \\\n","    next_singleton_ob_log_grad_list, num_evals, i_bias\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gma6Nles0yF"},"source":["## Function: analytic_Bi_adaptive_thinning"]},{"cell_type":"code","metadata":{"id":"zLocUuzptD2C"},"source":["# Let Lambda be the upper-bound intensity function.\n","# Let Lambda = max(0, Rho).\n","\n","def analytic_Bi_adaptive_thinning(x, v, array_for_lin_reg, lin_reg_delta_U_list, hat_beta_0, hat_beta_1, \n","                                  Sigma_cov_mat, prob_dist, num_lin_reg, k, d_geom, \n","                                  nature_of_noise='artificial_gaus', \n","                                  sequence_of_log_grad_lists = 'None', num_bisections = 17):\n","  \n","  # Debugging: inspect the magnitude of upper-bound intensity\n","#  print('beta_0: {}'.format(hat_beta_0))\n","#  print('beta_1: {}'.format(hat_beta_1))\n","#  print('Covariance matrix: {}'.format(Sigma_cov_mat))\n","\n","\n","  # i_bias indicates that bias is introduced because the upper-bound intensity goes below the the realised\n","  # intensity of the Cox Process.\n","  i_bias = 0\n","  num_evals_ADAPTIVE = 0\n","\n","\n","  # Step 1: make Sigma_cov_mat positive definite.\n","  epsilon = 0.0001\n","  Sigma_cov_mat[0,0] = Sigma_cov_mat[0,0] + epsilon\n","\n","\n","  # Step 2: define the Rho function\n","  Rho = lambda t: hat_beta_0 + hat_beta_1*t + k*np.sqrt(Sigma_cov_mat[0,0] + 2*Sigma_cov_mat[0,1]*t \n","                                                        + Sigma_cov_mat[1,1]*(t**2))\n","\n","#  print('beta 0: {}'.format(hat_beta_0))\n","#  print('beta 1: {}'.format(hat_beta_1))\n","#  print('Sigma: {}'.format(Sigma_cov_mat))\n","\n","\n","\n","\n","  # Step 3: find all global time t such that Rho(t)=0. There is an exception\n","  #         when A=0, B=0, C=0.\n","  A, B, C, candidates = quadratic_equation_candidates_only(hat_beta_0, hat_beta_1, Sigma_cov_mat, k)\n","  \n","  \n","\n","\n","\n","\n","\n","\n","  # Step 4: Lambda = max(0, Rho). We need to determine the sign of Rho in different regions of [0,t_geom],\n","  #         by picking a point (the variable test_point below) and evaluate Rho on it.\n","  t_geom = d_geom/np.linalg.norm(v)\n","\n","  if candidates == 'no_candidates':\n","    boundaries_and_zeroes = [0, t_geom]\n","  else:\n","    # a new condition of (hat_beta_0 + hat_beta_1*x < 0) is added to reject roots.\n","    filter_candidates = [x for x in candidates if (0 < x) and (x < t_geom) \n","                         and (hat_beta_0 + hat_beta_1*x < 0)]\n","    boundaries_and_zeroes = [0] + filter_candidates + [t_geom]\n","  \n","  B_and_Z_length = len(boundaries_and_zeroes)\n","  \n","  # 1 means that the region has a positive sign of Rho. 0 means otherwise.\n","  signs_of_regions = []\n","\n","  for i in range(B_and_Z_length - 1):\n","    test_point = 0.5*(boundaries_and_zeroes[i] + boundaries_and_zeroes[i+1])\n","\n","    if Rho(test_point) > 0:\n","      signs_of_regions.append(1)\n","    else:\n","      signs_of_regions.append(0)\n","\n","\n","\n","  # Step 5: Note that Rho can be integrated analytically. Hence, We need to construct an \n","  # anti-derivative for it.\n","  a = Sigma_cov_mat[1,1]\n","  b = Sigma_cov_mat[0,0] - (Sigma_cov_mat[0,1]**2)/Sigma_cov_mat[1,1]\n","  c = Sigma_cov_mat[0,1]/Sigma_cov_mat[1,1]\n","\n","  complicated_1 = lambda t: (t + c)*np.sqrt(a*((t + c)**2) + b)\n","  complicated_2 = lambda t: (b/np.sqrt(a))*np.log(np.sqrt(a)*np.sqrt(a*((t + c)**2) + b) + a*(t + c))  \\\n","  if (not b == 0)   else  0\n","\n","  Anti_derivative = lambda t: hat_beta_0*t + 0.5*hat_beta_1*(t**2) +\\\n","  0.5*k*(complicated_1(t) + complicated_2(t))\n","\n","\n","\n","  # Step 6: compute intensity in each region.\n","  intensities_of_individual_regions = []\n","  cumulative_intensities_of_each_region = [0]\n","  \n","  for i in range(B_and_Z_length - 1):\n","    if signs_of_regions[i] == 0:\n","      intensities_of_individual_regions.append(0)\n","      \n","      cumulative_intensities_of_each_region.append(cumulative_intensities_of_each_region[-1])\n","\n","    elif signs_of_regions[i] == 1:\n","      regional_intensity = (Anti_derivative(boundaries_and_zeroes[i+1]) - \n","                            Anti_derivative(boundaries_and_zeroes[i]))\n","      \n","      intensities_of_individual_regions.append(regional_intensity)\n","      cumulative_intensities_of_each_region.append(cumulative_intensities_of_each_region[-1] + \n","                                                   regional_intensity)\n","\n","  cumulative_intensities_of_each_region = cumulative_intensities_of_each_region[1:]\n","\n","\n","\n","\n","  # Step 7: Simulate the first arrival of the Cox Process via adaptive thinning on [0, t_geom].\n","  #         Note that it is possible that our upper-bound NHPP (or our Cox Process) has no \n","  #         arrivals at all.\n","\n","\n","  # We are going to simulate events of our upper-bound NHPP in [0, t_geom] until having an acceptance,\n","  # or until that the whole period [0, t_geom] has been simulated. We will accumulate the 'rejected' \n","  # inverse-transform intensities in the following variable.\n","  accumulated_intensity = 0\n","  termination = 0\n","  num_proposals = 1\n","\n","  while termination == 0:\n","    V = np.random.uniform()*(-1) + 1\n","    accumulated_intensity = accumulated_intensity - np.log(V)\n","\n","\n","\n","    # this is an indicator for absence of events in NHPP\n","    i_no_occurrence = 1\n","    ith_region = 0\n","\n","    while (i_no_occurrence == 1) and (ith_region <= (B_and_Z_length - 2)):\n","      if accumulated_intensity == cumulative_intensities_of_each_region[ith_region]:\n","        proposal_event_time = boundaries_and_zeroes[ith_region+1]\n","        i_no_occurrence = 0\n","    \n","      elif accumulated_intensity < cumulative_intensities_of_each_region[ith_region]:\n","        if ith_region == 0:\n","          remaining_intensity = accumulated_intensity\n","        \n","        elif ith_region > 0:\n","          remaining_intensity = accumulated_intensity - cumulative_intensities_of_each_region[ith_region-1]\n","      \n","        # We consider the following function to have the following interval domain:\n","        # [current_region_lower_bound, current_region_upper_bound]\n","        twisted_integral = lambda t: Anti_derivative(t) - Anti_derivative(boundaries_and_zeroes[ith_region])\\\n","        - remaining_intensity\n","      \n","        proposal_event_time = Bisection_Method_strict_increase(twisted_integral, boundaries_and_zeroes[ith_region], \n","                                                               boundaries_and_zeroes[ith_region+1], num_bisections)\n","        i_no_occurrence = 0\n","      \n","      ith_region = ith_region + 1\n","    \n","    if i_no_occurrence == 1:\n","      event_time = 'no_events'\n","      delta_U_tilde = 'N/A'\n","      G_tilde = 'N/A'\n","      c_t_squared = 'N/A'\n","      log_grad_list = 'N/A'\n","      termination = 1\n","  \n","    elif i_no_occurrence == 0:\n","      # acceptance/rejection step\n","      proposal_intensity = max(0, Rho(proposal_event_time))\n","      proposal_location = x + v*proposal_event_time\n","\n","\n","      # A situation that is unlikely to occur in the real computational environment: proposal_event_time\n","      # is one of those times used for performing linear regression [0, t_geom/(k-1), ..., t_geom].\n","      # If this really occurs, then we should not re-evaluate noisy gradient and G_tilde again.\n","      # Inside each nature_of_noise case, I tackle this unlikely situation, because I feel responsible\n","      # for coding in more things for the research of the PINTS group.\n","      \n","      if nature_of_noise == 'artificial_gaus':\n","\n","        # i_lin_reg_time is an indicator for whether the proposal time is a time instant\n","        # used in performing linear regression.\n","        i_lin_reg_time = 0\n","        lin_reg_time_list = array_for_lin_reg[:,0]\n","        \n","        # debugging\n","#        if (not np.array_equal(lin_reg_time_list, np.arange(num_lin_reg)*t_geom/(num_lin_reg-1))):\n","#          print('error in analytic_Bi: timing of linear regression')\n","#          print('lin_reg_time_list is: {}'.format(lin_reg_time_list))\n","#          print('The supposed-to-be-correct is: {}'.format(np.arange(num_lin_reg)*t_geom/(num_lin_reg-1)))\n","#        if (not len(lin_reg_time_list) == num_lin_reg):\n","#          print('error in analytic_Bi: length of timing of linear regression')\n","      \n","        for i in range(num_lin_reg):\n","          if proposal_event_time == lin_reg_time_list[i]:\n","            i_lin_reg_time = ['revisited ith', i]\n","\n","        if (not i_lin_reg_time == 0):\n","          lin_reg_time, G_tilde, c_t_squared = array_for_lin_reg[i_lin_reg_time[1]]\n","          delta_U_tilde = lin_reg_delta_U_list[i_lin_reg_time[1]]\n","\n","          log_grad_list = 'N/A'\n","        \n","\n","        elif i_lin_reg_time == 0:\n","          delta_U_tilde, G_tilde, c_t_squared, log_grad_list =\\\n","          eval_G_tilde_plus_emp_var(proposal_location, v, prob_dist)\n","\n","          num_evals_ADAPTIVE = num_evals_ADAPTIVE + 1\n","\n","        real_intensity = max(0, G_tilde)\n","\n","        if real_intensity > proposal_intensity:\n","          i_bias = 1\n","        \n","        U = np.random.uniform()\n","        if U <= probability_of_acceptance(real_intensity, proposal_intensity):\n","          event_time = proposal_event_time\n","          termination = 1\n","\n","      \n","\n","\n","      elif nature_of_noise == 'subsampling':\n","\n","        i_lin_reg_time = 0\n","        lin_reg_time_list = array_for_lin_reg[:,0]\n","        \n","        # debugging\n","#        if (not lin_reg_time_list == np.arange(num_lin_reg)*t_geom/(num_lin_reg-1)):\n","#          print('error in analytic_Bi: timing of linear regression')\n","#          print('lin_reg_time_list is: {}'.format(lin_reg_time_list))\n","#        if (not len(lin_reg_time_list) == num_lin_reg):\n","#          print('error in analytic_Bi: length of timing of linear regression')\n","      \n","        for i in range(num_lin_reg):\n","          if proposal_event_time == lin_reg_time_list[i]:\n","            i_lin_reg_time = ['revisited ith', i]\n","\n","        if (not i_lin_reg_time == 0):\n","          lin_reg_time, G_tilde, c_t_squared = array_for_lin_reg[i_lin_reg_time[1]]\n","          delta_U_tilde = lin_reg_delta_U_list[i_lin_reg_time[1]]\n","\n","          log_grad_list = sequence_of_log_grad_lists[i_lin_reg_time[1]]\n","        \n","\n","        elif i_lin_reg_time == 0:\n","          delta_U_tilde, G_tilde, c_t_squared, log_grad_list =\\\n","          eval_G_tilde_plus_emp_var(proposal_location, v, prob_dist)\n","\n","          num_evals_ADAPTIVE = num_evals_ADAPTIVE + 1\n","\n","        real_intensity = max(0, G_tilde)\n","        \n","        if real_intensity > proposal_intensity:\n","          i_bias = 1\n","\n","        U = np.random.uniform()\n","        if U <= probability_of_acceptance(real_intensity, proposal_intensity):\n","          event_time = proposal_event_time\n","          termination = 1\n","   \n","    if num_proposals % 5000 == 0:\n","      print('number of proposals: {}'.format(num_proposals))\n","    num_proposals = num_proposals + 1\n","\n","\n","\n","  return event_time, delta_U_tilde, G_tilde, c_t_squared, log_grad_list, num_evals_ADAPTIVE, i_bias\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqiZMh_8hsUb"},"source":["## Function: Roots of Quadratic Equations"]},{"cell_type":"code","metadata":{"id":"bEoWpwLhhGje"},"source":["def quadratic_equation_candidates_only(hat_beta_0, hat_beta_1, Sigma_cov_mat, k):\n","\n","  A = (k**2)*Sigma_cov_mat[1,1] - hat_beta_1**2\n","  B = 2*((k**2)*Sigma_cov_mat[0,1] - hat_beta_0*hat_beta_1)\n","  C = (k**2)*Sigma_cov_mat[0,0] - hat_beta_0**2\n","\n","  if abs(A) < 10**(-10):\n","    A = 0\n","  if abs(B) < 10**(-10):\n","    B = 0\n","  if abs(C) < 10**(-10):\n","    C = 0\n","\n","\n","  if A==0 and B==0 and C==0:\n","#    print('1')\n","    candidates = [-hat_beta_0/hat_beta_1]\n","\n","\n","  elif A==0 and B==0 and (not C==0):\n","#    print('2')\n","    candidates = 'no_candidates'\n","\n","\n","  elif A==0 and (not B==0) and (not C==0):\n","#    print('3')\n","    # check if (-C/B) really produces zero intensity\n","    if hat_beta_0 + hat_beta_1*(-C/B) < 0:\n","      candidates = [-C/B]\n","    else:\n","      candidates = 'no_candidates'\n","  \n","\n","  elif A==0 and (not B==0) and C==0:\n","#    print('4')\n","    candidates = 'no_candidates'\n","\n","\n","\n","  else:      # this means that A is non-zero:\n","#    print('5')\n","    discriminant = B**2 - 4*A*C\n","    \n","    if discriminant < 0:\n","#      print('5.1')\n","      candidates = 'no_candidates'\n","    \n","    elif discriminant == 0:\n","#      print('5.2')\n","      sol = -B/(2*A)\n","      if hat_beta_0 + hat_beta_1*(sol) < 0:\n","        candidates = [sol]\n","      else:\n","        candidates = 'no_candidates'\n","\n","\n","\n","    \n","    elif discriminant > 0:\n","#      print('5.3')\n","      sol_1 = -B/(2*A) - np.sqrt(discriminant)/(2*A)\n","      sol_2 = -B/(2*A) + np.sqrt(discriminant)/(2*A)\n","\n","      if hat_beta_0 + hat_beta_1*(sol_1) < 0 and hat_beta_0 + hat_beta_1*(sol_2) < 0:\n","        candidates = [sol_1, sol_2]\n","\n","      elif hat_beta_0 + hat_beta_1*(sol_1) < 0 and (not hat_beta_0 + hat_beta_1*(sol_2) < 0):\n","        candidates = [sol_1]\n","\n","      elif (not hat_beta_0 + hat_beta_1*(sol_1) < 0) and hat_beta_0 + hat_beta_1*(sol_2) < 0:\n","        candidates = [sol_2]\n","\n","      else:\n","        candidates = 'no_candidates'\n","\n","\n","  return A, B, C, candidates\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_3UQjzhCJE2"},"source":["## Function: Bisection Method"]},{"cell_type":"code","metadata":{"id":"KDIwT202COg4"},"source":["# num_iters is the number of bisections to be performed.\n","\n","\n","def Bisection_Method_strict_increase(F, a, b, num_iters):\n","  exact_sol = 'bla'\n","  i_exact_sol = 0\n","  \n","  if F(a) == 0:\n","    exact_sol = a\n","    i_exact_sol = 1\n","  \n","  elif F(b) == 0:\n","    exact_sol = b\n","    i_exact_sol = 1\n","  \n","  else:\n","    lower_bound = a\n","    upper_bound = b\n","\n","    for i in range(num_iters):\n","      mid_point = 0.5*(lower_bound + upper_bound)\n","\n","      if F(mid_point) == 0:\n","        exact_sol = mid_point\n","        i_exact_sol = 1\n","        break\n","\n","      elif F(mid_point) < 0:\n","        lower_bound = mid_point\n","      \n","      elif F(mid_point) > 0:\n","        upper_bound = mid_point\n","    \n","    approx_sol = 0.5*(lower_bound + upper_bound)\n","\n","  \n","  if i_exact_sol == 1:\n","    return exact_sol\n","\n","  elif i_exact_sol == 0:\n","    return approx_sol\n","\n"],"execution_count":null,"outputs":[]}]}